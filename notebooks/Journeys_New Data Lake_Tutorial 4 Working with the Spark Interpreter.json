{"paragraphs":[{"text":"%md\n# Tutorial 4: Working with the Spark Interpeter\n\nThis tutorial was built for BDC version 18.1.4 as part of the New Data Lake User Journey: <a href=\"https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake\" target=\"_blank\">here</a>.  Questions and feedback about the tutorial: <david.bayard@oracle.com>\n\n    Be sure you previously ran the Tutorial: \"Citi Bike New York Demo Introduction and Setup\"\n\nThis tutorial will illustrate how to run Spark interpreter to define a temporary Spark SQL table against a CSV file and then query it.\n\n## Contents\n\n+ About Spark\n+ Reading the data and registering as a Spark SQL table\n+ Querying bike trip information\n+ Next Steps\n\nAs a reminder, the documentation for BDC can be found: <a href=\"http://docs.oracle.com/cloud/latest/big-data-compute-cloud/index.html\" target=\"_blank\">here</a>","dateUpdated":"2018-04-25T21:12:58+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Tutorial 4: Working with the Spark Interpeter</h1>\n<p>This tutorial was built for BDC version 18.1.4 as part of the New Data Lake User Journey: <a href=\"https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake\" target=\"_blank\">here</a>. Questions and feedback about the tutorial: <a href=\"mailto:&#100;a&#118;&#x69;&#x64;&#x2e;&#x62;&#97;&#x79;&#x61;r&#100;&#x40;&#111;r&#97;&#99;&#108;e&#46;c&#x6f;&#109;\">&#100;a&#118;&#x69;&#x64;&#x2e;&#x62;&#97;&#x79;&#x61;r&#100;&#x40;&#111;r&#97;&#99;&#108;e&#46;c&#x6f;&#109;</a></p>\n<pre><code>Be sure you previously ran the Tutorial: &quot;Citi Bike New York Demo Introduction and Setup&quot;\n</code></pre>\n<p>This tutorial will illustrate how to run Spark interpreter to define a temporary Spark SQL table against a CSV file and then query it.</p>\n<h2>Contents</h2>\n<ul>\n  <li>About Spark</li>\n  <li>Reading the data and registering as a Spark SQL table</li>\n  <li>Querying bike trip information</li>\n  <li>Next Steps</li>\n</ul>\n<p>As a reminder, the documentation for BDC can be found: <a href=\"http://docs.oracle.com/cloud/latest/big-data-compute-cloud/index.html\" target=\"_blank\">here</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1524690778501_-344722918","id":"20170414-131903_889251720","dateCreated":"2018-04-25T21:12:58+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:460"},{"text":"%md\n# About Spark and Spark SQL\n\nBDCS-CE version 17.4.1 comes with Spark version 2.1 and Scala version 2.11.  This BDCS-CE version supplies Zeppelin interpreters for Spark(Scala), Spark SQL, SparkR, and pySpark.  This tutorial will give you examples using the Spark(scala) and SparkSQL interpreters.\n\nThe tutorial assumes you have a basic knowledge about Spark.  To learn more about Spark, check out <a href=\"https://spark.apache.org/docs/2.1.0/quick-start.html\" target=\"_blank\">Spark Quick Start</a> and <a href=\"https://spark.apache.org/docs/2.1.0/sql-programming-guide.html\" target=\"_blank\">Spark SQL Programming Guide</a>\n\n","dateUpdated":"2018-04-25T21:12:58+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>About Spark and Spark SQL</h1>\n<p>BDCS-CE version 17.4.1 comes with Spark version 2.1 and Scala version 2.11. This BDCS-CE version supplies Zeppelin interpreters for Spark(Scala), Spark SQL, SparkR, and pySpark. This tutorial will give you examples using the Spark(scala) and SparkSQL interpreters.</p>\n<p>The tutorial assumes you have a basic knowledge about Spark. To learn more about Spark, check out <a href=\"https://spark.apache.org/docs/2.1.0/quick-start.html\" target=\"_blank\">Spark Quick Start</a> and <a href=\"https://spark.apache.org/docs/2.1.0/sql-programming-guide.html\" target=\"_blank\">Spark SQL Programming Guide</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1524690778502_-343568672","id":"20170616-103849_1176559517","dateCreated":"2018-04-25T21:12:58+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:461"},{"text":"%md\n## Reading the data and registering as a Spark SQL table\n\nThe next step is to use Spark to read our bike data CSV file that we uploaded to the Object Store.  Once we read the CSV into a Spark Data Frame, we will ask Spark to cache the data in memory.  Then we will register the data frame as a Spark SQL temp table.\n\nYou can review the Spark SQL programming guide for a refresher about Data Frames and Temporary Tables: <a href=\"https://spark.apache.org/docs/2.1.0/sql-programming-guide.html\" target=\"_blank\">Spark SQL Programming Guide</a>\n\n","dateUpdated":"2018-04-25T21:12:58+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Reading the data and registering as a Spark SQL table</h2>\n<p>The next step is to use Spark to read our bike data CSV file that we uploaded to the Object Store. Once we read the CSV into a Spark Data Frame, we will ask Spark to cache the data in memory. Then we will register the data frame as a Spark SQL temp table.</p>\n<p>You can review the Spark SQL programming guide for a refresher about Data Frames and Temporary Tables: <a href=\"https://spark.apache.org/docs/2.1.0/sql-programming-guide.html\" target=\"_blank\">Spark SQL Programming Guide</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1524690778502_-343568672","id":"20170417-090240_1793194469","dateCreated":"2018-04-25T21:12:58+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:462"},{"title":"Spark Scala to read CSV and register as a temp view (takes 1-3 minutes.  or longer see comments in code)","text":"%spark\n\n//if you created the smallest BDC environment (2 OCPU) and just finished the previous lab, this step will take 6 minutes or so.\n//this is because when you ran the last lab, you started a hive/tez session which will remain open for a few minutes and will use/block resources needed by spark\n//after a few minutes, hive/tez will close its session and this spark code will run.  You could go to the Jobs tab and manually abort the hive session, or just relax and wait.\n\n\n\n//a previous tutorial placed the csv file into your Object Store citibike container\n//notice the use of the swift://CONTAINER.default/ syntax\nval Container = \"journeyC\"\nval Directory = \"citibike\"\n\nsqlContext.setConf(\"spark.sql.shuffle.partitions\", \"4\")\n\nvar df: org.apache.spark.sql.DataFrame = null\n\nif( \"oci://test01bdc01@orasejapan\".contains(\"swift\")  ){\n         println(\"Running on OCI-C\");\n   //We will use the bdfs (alluxio) cached file system to access our object store data...\n   //IMPORTANT NOTE FOR PEOPLE USING BDC 18.2.4.  THERE IS BUG 28012306 (should be fixed with 18.2.6) THAT BREAKS BDFS: FILEPATHS.\n   //If you get an error about bdfs and are using 18.2.4, you need to remove \".default/\" from the end of the alluxio.underfs.address parameter.  Use Ambari and you will find this parameter\n   //under the Alluxio Advanced alluxio-site configurations.\n   df = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferSchema\",\"true\").load(\"bdfs://localhost:19998/\"+Directory+\"/raw/201612-citibike-tripdata.csv\")\n} else {\n         println(\"Running on OCI\");\n   df = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").load(\"oci://test01bdc01@orasejapan/\"+Directory+\"/raw/201612-citibike-tripdata.csv\")\n}\n\n\n// If you get this error message:\n// java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n// Then go to the Settings tab, then click on Notebook.  Then restart the Notebook.  This will restart your SparkContext\n\n\nprintln(\"Here is the schema detected from the CSV\")\ndf.printSchema()\nprintln(\"..\")\n\nprintln(\"# of rows: %s\".format(\n  df.count() \n)) \nprintln(\"..\")\n\ndf.createOrReplaceTempView(\"bike_trips_temp\")\nprintln(\"done\")","user":"anonymous","dateUpdated":"2018-06-02T15:25:39+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{"0":{"graph":{"mode":"table","height":588,"optionOpen":false}}},"graph":{"mode":"table","height":247,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{"container":"dcb-bdcs-apr12","CONTAINER":"citibike","FILENAME":"201612-citibike-tripdata"},"forms":{}},"apps":[],"jobName":"paragraph_1524690778503_-343953421","id":"20170414-134031_1271833288","dateCreated":"2018-04-25T21:12:58+0000","dateStarted":"2018-06-02T15:25:39+0000","dateFinished":"2018-06-02T15:26:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:463","errorMessage":""},{"text":"%md\n# Querying the bike trip information\n\nNow we can show some examples of querying our bike_trips table.  You will need to have first run the above paragraph to ensure that the temporary table bike_trips_temp is registered in your current Spark Session.","dateUpdated":"2018-04-25T21:12:58+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Querying the bike trip information</h1>\n<p>Now we can show some examples of querying our bike_trips table. You will need to have first run the above paragraph to ensure that the temporary table bike_trips_temp is registered in your current Spark Session.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1524690778503_-343953421","id":"20170417-093337_291620887","dateCreated":"2018-04-25T21:12:58+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:464"},{"title":"Trips by Gender","text":"%sql\nselect \n case when a.gender=1 then 'Male' when a.gender=2 then 'Female' else 'unknown' end gender ,\n        a.trip_count \nfrom (select gender, count(*) trip_count from bike_trips_temp\ngroup by gender) a","dateUpdated":"2018-04-25T21:12:58+0000","config":{"editorSetting":{"language":"sql"},"colWidth":6,"editorMode":"ace/mode/sql","title":true,"results":[{"graph":{"mode":"pieChart","height":294,"optionOpen":false,"keys":[{"name":"gender","index":0,"aggr":"sum"}],"values":[{"name":"trip_count","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"gender","index":0,"aggr":"sum"}}}}],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1524690778503_-343953421","id":"20170417-095126_1698083225","dateCreated":"2018-04-25T21:12:58+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:465"},{"title":"Trips by Day of Month","text":"%sql\nselect dayofmonth, count(*)\nfrom (select date_format(`Start Time`,\"H\") hour,\n date_format(`Start Time`,\"E\") dayofweek,\n date_format(`Start Time`,\"d\") dayofmonth\nfrom bike_trips_temp) bike_times\ngroup by dayofmonth","dateUpdated":"2018-04-25T21:12:58+0000","config":{"editorSetting":{"language":"sql"},"colWidth":6,"editorMode":"ace/mode/sql","title":true,"results":[{"graph":{"mode":"lineChart","height":314,"optionOpen":false,"keys":[{"name":"dayofmonth","index":0,"aggr":"sum"}],"values":[{"name":"count(1)","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"dayofmonth","index":0,"aggr":"sum"},"yAxis":{"name":"_c1","index":1,"aggr":"sum"}},"forceY":true,"setting":{"lineChart":{}},"commonSetting":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1524690778504_-345877165","id":"20170417-095623_1767722062","dateCreated":"2018-04-25T21:12:58+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:466"},{"title":"Trips by Day of Week and Gender","text":"%sql\nselect dayofweek, count(*)\nfrom (select date_format(`Start Time`,\"H\") hour,\n date_format(`Start Time`,\"E\") dayofweek,\n date_format(`Start Time`,\"d\") dayofmonth,\n case when gender=1 then 'Male' when gender=2 then 'Female' else 'unknown' end gender \nfrom bike_trips_temp) bike_times\nwhere (gender=\"${gender=Male,Male|Female|unknown}\" )\ngroup by dayofweek","dateUpdated":"2018-04-25T21:12:58+0000","config":{"editorSetting":{"language":"sql"},"colWidth":6,"editorMode":"ace/mode/sql","title":true,"results":[{"graph":{"mode":"pieChart","height":300,"optionOpen":false,"keys":[{"name":"dayofweek","index":0,"aggr":"sum"}],"values":[{"name":"count(1)","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"dayofweek","index":0,"aggr":"sum"}},"setting":{"pieChart":{}},"commonSetting":{}}}],"enabled":true},"settings":{"params":{"gender":"Male"},"forms":{"gender":{"name":"gender","defaultValue":"Male","options":[{"value":"Male","$$hashKey":"object:894"},{"value":"Female","$$hashKey":"object:895"},{"value":"unknown","$$hashKey":"object:896"}],"hidden":false,"$$hashKey":"object:887"}}},"apps":[],"jobName":"paragraph_1524690778504_-345877165","id":"20170417-101619_429877425","dateCreated":"2018-04-25T21:12:58+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:467"},{"title":"Bike Trips by Hour by day of week","text":"%sql\nselect dayofweek, hour, count(*)\nfrom (select date_format(`Start Time`,\"H\") hour,\n date_format(`Start Time`,\"E\") dayofweek,\n date_format(`Start Time`,\"d\") dayofmonth,\n case when gender=1 then 'Male' when gender=2 then 'Female' else 'unknown' end gender \nfrom bike_trips_temp) bike_times\nwhere (gender=\"${gender=Male,Male|Female|unknown}\" )\ngroup by dayofweek, hour","user":"anonymous","dateUpdated":"2018-04-26T14:18:45+0000","config":{"editorSetting":{"language":"sql"},"colWidth":6,"editorMode":"ace/mode/sql","title":true,"results":[{"graph":{"mode":"lineChart","height":300,"optionOpen":false,"keys":[{"name":"hour","index":1,"aggr":"sum"}],"values":[{"name":"count(1)","index":2,"aggr":"sum"}],"groups":[{"name":"dayofweek","index":0,"aggr":"sum"}],"scatter":{"xAxis":{"name":"dayofweek","index":0,"aggr":"sum"},"yAxis":{"name":"hour","index":1,"aggr":"sum"}},"forceY":true,"lineWithFocus":false,"setting":{"lineChart":{}},"commonSetting":{}}}],"enabled":true},"settings":{"params":{"gender":"Male"},"forms":{"gender":{"name":"gender","defaultValue":"Male","options":[{"value":"Male","$$hashKey":"object:1798"},{"value":"Female","$$hashKey":"object:1799"},{"value":"unknown","$$hashKey":"object:1800"}],"hidden":false,"$$hashKey":"object:1792"}}},"apps":[],"jobName":"paragraph_1524690778504_-345877165","id":"20170414-134451_506665191","dateCreated":"2018-04-25T21:12:58+0000","dateStarted":"2018-04-26T14:18:45+0000","dateFinished":"2018-04-26T14:19:03+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:468","errorMessage":""},{"text":"%md\n# Next Steps\n\nSo far, we have downloaded Citi Bike data and stored it into the Object Store.  Then, we configured Spark to be able to work with CSV files.  Then, we read in the data and defined a Spark SQL temporary table with it.  Finally, we demonstrated a number of different queries.  Did you notice any patterns?  For instance, that men use Citi Bikes more than women?  That on workdays (Mon-Fri) there is a peak around 8am and 5pm, but that peak does not exist on Saturday and Sunday?   \n\nHere are some suggested next steps:\n\n+ Run the Demonstration Presidental Speeches with Spark and Spark SQL\n+ Explore the Tutorial Spark and Maps in Zeppelin\n+ Upload some of your own data to the Object Store and experiment with Spark and Spark SQL\n","dateUpdated":"2018-04-25T21:12:58+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Next Steps</h1>\n<p>So far, we have downloaded Citi Bike data and stored it into the Object Store. Then, we configured Spark to be able to work with CSV files. Then, we read in the data and defined a Spark SQL temporary table with it. Finally, we demonstrated a number of different queries. Did you notice any patterns? For instance, that men use Citi Bikes more than women? That on workdays (Mon-Fri) there is a peak around 8am and 5pm, but that peak does not exist on Saturday and Sunday? </p>\n<p>Here are some suggested next steps:</p>\n<ul>\n  <li>Run the Demonstration Presidental Speeches with Spark and Spark SQL</li>\n  <li>Explore the Tutorial Spark and Maps in Zeppelin</li>\n  <li>Upload some of your own data to the Object Store and experiment with Spark and Spark SQL</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1524690778505_-346261914","id":"20170417-103925_248941849","dateCreated":"2018-04-25T21:12:58+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:469"},{"text":"%md\n# Extra Credit - Saving our temporary Spark SQL table as a permanent Hive table\n\nThe next few paragraphs show you how you can save a copy of the Spark SQL temporary table as a new permament Hive table.  This might be useful if you want to use BI tools, like Oracle Data Visualization Desktop, to query the permanent table.\n\nThe only trick is that Hive doesn't like spaces in column names, so we rename our columns in our Create Table as Select statement below.","dateUpdated":"2018-04-25T21:12:58+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Extra Credit - Saving our temporary Spark SQL table as a permanent Hive table</h1>\n<p>The next few paragraphs show you how you can save a copy of the Spark SQL temporary table as a new permament Hive table. This might be useful if you want to use BI tools, like Oracle Data Visualization Desktop, to query the permanent table.</p>\n<p>The only trick is that Hive doesn&rsquo;t like spaces in column names, so we rename our columns in our Create Table as Select statement below.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1524690778505_-346261914","id":"20170505-092652_1652882871","dateCreated":"2018-04-25T21:12:58+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:470"},{"title":"Query to list our hive tables BEFORE","text":"%sql\nshow tables","dateUpdated":"2018-04-25T21:12:58+0000","config":{"editorSetting":{"language":"sql"},"colWidth":12,"editorMode":"ace/mode/sql","title":true,"results":[{"graph":{"mode":"table","height":174,"optionOpen":false,"keys":[{"name":"tableName","index":0,"aggr":"sum"}],"values":[{"name":"isTemporary","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"tableName","index":0,"aggr":"sum"},"yAxis":{"name":"isTemporary","index":1,"aggr":"sum"}}}}],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1524690778506_-345107667","id":"20170505-092452_1351056533","dateCreated":"2018-04-25T21:12:58+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471"},{"title":"HiveQL to drop the table (in case you already ran the next step)","text":"%sql\ndrop table bike_trips_parquet","dateUpdated":"2018-04-25T21:12:58+0000","config":{"editorSetting":{"language":"sql"},"colWidth":12,"editorMode":"ace/mode/sql","title":true,"results":[],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1524690778506_-345107667","id":"20170612-094821_740135087","dateCreated":"2018-04-25T21:12:58+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:472"},{"title":"HiveQL to create a permanent Hive table from our SparkSQL temporary view (this will have no output)","text":"%sql\ncreate table bike_trips_parquet\nstored as parquet\nas select `Trip Duration` TRIPDURATION,\n`Start Time` STARTTIME,\n`Stop Time` STOPTIME,\n`Start Station ID` STARTSTATIONID,\n`Start Station Name` STARTSTATIONNAME,\n`Start Station Latitude` STARTSTATIONLATITUDE,\n`Start Station Longitude` STARTSTATIONLONGITUDE,\n`End Station ID` ENDSTATIONID,\n`End Station Name` ENDSTATIONNAME,\n`End Station Latitude` ENDSTATIONLATITUDE,\n`End Station Longitude` ENDSTATIONLONGITUDE,\n`Bike ID` BIKEID,\n`User Type` USERTYPE,\n`Birth Year` BIRTHYEAR,\n`Gender` GENDER\n from bike_trips_temp","user":"anonymous","dateUpdated":"2018-04-26T14:23:24+0000","config":{"editorSetting":{"language":"sql"},"colWidth":12,"editorMode":"ace/mode/sql","title":true,"results":[],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1524690778506_-345107667","id":"20170414-151953_1584996855","dateCreated":"2018-04-25T21:12:58+0000","dateStarted":"2018-04-26T14:23:24+0000","dateFinished":"2018-04-26T14:23:48+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:473","errorMessage":""},{"title":"Query to show our new permanent table in action","text":"%sql\nselect * from bike_trips_parquet limit 5","dateUpdated":"2018-04-25T21:12:58+0000","config":{"editorSetting":{"language":"sql"},"colWidth":12,"editorMode":"ace/mode/sql","title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"tripduration","index":0,"aggr":"sum"}],"values":[{"name":"starttime","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"tripduration","index":0,"aggr":"sum"},"yAxis":{"name":"starttime","index":1,"aggr":"sum"}}}}],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1524690778507_-345492416","id":"20170503-195617_839333002","dateCreated":"2018-04-25T21:12:58+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:474"},{"title":"Query to list our Hive tables AFTER","text":"%sql\nshow tables","dateUpdated":"2018-04-25T21:12:58+0000","config":{"editorSetting":{"language":"sql"},"colWidth":12,"editorMode":"ace/mode/sql","title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"tableName","index":0,"aggr":"sum"}],"values":[{"name":"isTemporary","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"tableName","index":0,"aggr":"sum"},"yAxis":{"name":"isTemporary","index":1,"aggr":"sum"}}}}],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1524690778507_-345492416","id":"20170503-195744_526374432","dateCreated":"2018-04-25T21:12:58+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:475"},{"text":"%md\n### Change Log\nJune 2, 2018 - added a note about Bug 28012306 affecting BDFS in 18.2.4\nMarch 6, 2018 - tested with OCI and 18.1.4\nNovember 15, 2017 - replaced registerTempTable with createOrReplaceTempView\nOctober 11, 2017 - Added spark.sql.shuffle.partitions=4 \nSeptember 7, 2017 - Confirmed it works with 17.3.5-20.  Switched filepath to bdfs\nAugust 23, 2017 - Minor tweaks\nAugust 13, 2017 - Confirmed it works with 17.3.3-20.\nAugust 11, 2017 - Journey v2.  Confirmed it worked with Spark2.1\nJuly 28, 2017 - Confirmed it works with 17.3.1-20.","dateUpdated":"2018-06-02T15:26:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Change Log</h3>\n<p>June 2, 2018 - added a note about Bug 28012306 affecting BDFS in 18.2.4<br/>March 6, 2018 - tested with OCI and 18.1.4<br/>November 15, 2017 - replaced registerTempTable with createOrReplaceTempView<br/>October 11, 2017 - Added spark.sql.shuffle.partitions=4<br/>September 7, 2017 - Confirmed it works with 17.3.5-20. Switched filepath to bdfs<br/>August 23, 2017 - Minor tweaks<br/>August 13, 2017 - Confirmed it works with 17.3.3-20.<br/>August 11, 2017 - Journey v2. Confirmed it worked with Spark2.1<br/>July 28, 2017 - Confirmed it works with 17.3.1-20.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1524690778507_-345492416","id":"20170614-163657_1564876178","dateCreated":"2018-04-25T21:12:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:476","user":"anonymous","dateFinished":"2018-06-02T15:26:45+0000","dateStarted":"2018-06-02T15:26:45+0000"},{"text":"%md\n","dateUpdated":"2018-04-25T21:12:58+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":"true"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1524690778507_-345492416","id":"20170728-131606_1414555962","dateCreated":"2018-04-25T21:12:58+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:477"}],"name":"Journeys/New Data Lake/Tutorial 4 Working with the Spark Interpreter","id":"2DC1VYFRX","angularObjects":{"2DD6NF1GD:shared_process":[],"2DEYRE5Z7:shared_process":[],"2DBHU1VH8:shared_process":[],"2DBJEDYUY:shared_process":[],"2DB4EM98X:shared_process":[],"2DBXRFZ9E:shared_process":[],"2DCJFVXZH:shared_process":[],"2C4U48MY3_spark2:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}