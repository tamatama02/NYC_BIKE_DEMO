{"paragraphs":[{"text":"%md\n# Example: Querying a Temporary View via JDBC\n\nThis tutorial was built for BDCS-CE version 17.4.1 and OEHCS 0.10.2 as part of the New Data Lake User Journey: <a href=\"https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake\" target=\"_blank\">here</a>.  Questions and feedback about the tutorial: <david.bayard@oracle.com>\n\n    Please ensure that you have run the \"Citi Bike Introduction and Setup\" tutorial and the \"Streaming Tutorial 1: Working with OEHCS and Spark Streaming\" first.\n\n\n\nAs a reminder, the documentation for BDCS-CE can be found <a href=\"https://docs.oracle.com/cloud/latest/big-data-compute-cloud/index.html\" target=\"_new\">here</a>\n\n\n    Please run these paragraphs one at a time.  It will not work if you try to run the entire note all at once.\n","dateUpdated":"2018-08-15T14:32:52+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Example: Querying a Temporary View via JDBC</h1>\n<p>This tutorial was built for BDCS-CE version 17.4.1 and OEHCS 0.10.2 as part of the New Data Lake User Journey: <a href=\"https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake\" target=\"_blank\">here</a>. Questions and feedback about the tutorial: <a href=\"mailto:&#x64;&#x61;&#118;&#105;d&#x2e;&#x62;a&#121;&#x61;&#x72;&#100;&#64;&#111;&#114;&#97;&#x63;&#x6c;&#101;&#46;&#99;&#111;&#x6d;\">&#x64;&#x61;&#118;&#105;d&#x2e;&#x62;a&#121;&#x61;&#x72;&#100;&#64;&#111;&#114;&#97;&#x63;&#x6c;&#101;&#46;&#99;&#111;&#x6d;</a></p>\n<pre><code>Please ensure that you have run the &quot;Citi Bike Introduction and Setup&quot; tutorial and the &quot;Streaming Tutorial 1: Working with OEHCS and Spark Streaming&quot; first.\n</code></pre>\n<p>As a reminder, the documentation for BDCS-CE can be found <a href=\"https://docs.oracle.com/cloud/latest/big-data-compute-cloud/index.html\" target=\"_new\">here</a></p>\n<pre><code>Please run these paragraphs one at a time.  It will not work if you try to run the entire note all at once.\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1534343572582_-589717078","id":"20170419-090208_164595531","dateCreated":"2018-08-15T14:32:52+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:418"},{"text":"%md\n# About this Example\n\nThis example shows how you can start a HiveThriftServer2 in the same Spark application/job as you use with the Zeppelin notebook.  One of the things this allows you to do is to connect to the HiveThriftServer2 via JDBC and see the Temporary View objects you've created.\n\nIn this example, we will create a real-time Temporary View via Spark Streaming and then show how you can query it via JDBC.\n\nOur HiveThriftServer2 will be setup to run on port 10032 and use the binary transport mode.\n","dateUpdated":"2018-08-15T14:32:52+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>About this Example</h1>\n<p>This example shows how you can start a HiveThriftServer2 in the same Spark application/job as you use with the Zeppelin notebook. One of the things this allows you to do is to connect to the HiveThriftServer2 via JDBC and see the Temporary View objects you&rsquo;ve created.</p>\n<p>In this example, we will create a real-time Temporary View via Spark Streaming and then show how you can query it via JDBC.</p>\n<p>Our HiveThriftServer2 will be setup to run on port 10032 and use the binary transport mode.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1534343572583_-590101827","id":"20170419-090743_1265713500","dateCreated":"2018-08-15T14:32:52+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:419"},{"text":"%md\n# More background info\n\nThink of Spark in terms of applications.  Each application knows nothing about the other.  In BDCS-CE, out of the box you will see 2 applications (also called Jobs in the BDCS-CE console)\n\nOne is called \"Zeppelin\" which is all the spark code you run inside the Zeppelin notebook.  If you do the New Data Lake journey which has lots of spark/scala notebook paragraphs, this is where the spark/scala code runs.\n\nThe other application is called \"Thrift JDBC/ODBC Server\" which is an application that does nothing other than run the \"HiveThriftServer2\" process and is what is typically used for JDBC & ODBC connections.\n\nMost likely, your code to create the temporary table runs in \"Zeppelin\" but DVD is speaking to \"Thrift JDBC/ODBC Server\".  And \"Thrift JDBC/ODBC Server\" won't see your temporary view in \"Zeppelin\".\n\nTo expose a Temporary View via JDBC, you want to create your temporary view and run the HiveThriftServer2 in the same Spark application.  In this example, we will do so in the \"Zeppelin\" Spark application. ","dateUpdated":"2018-08-15T14:32:52+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>More background info</h1>\n<p>Think of Spark in terms of applications. Each application knows nothing about the other. In BDCS-CE, out of the box you will see 2 applications (also called Jobs in the BDCS-CE console)</p>\n<p>One is called &ldquo;Zeppelin&rdquo; which is all the spark code you run inside the Zeppelin notebook. If you do the New Data Lake journey which has lots of spark/scala notebook paragraphs, this is where the spark/scala code runs.</p>\n<p>The other application is called &ldquo;Thrift JDBC/ODBC Server&rdquo; which is an application that does nothing other than run the &ldquo;HiveThriftServer2&rdquo; process and is what is typically used for JDBC &amp; ODBC connections.</p>\n<p>Most likely, your code to create the temporary table runs in &ldquo;Zeppelin&rdquo; but DVD is speaking to &ldquo;Thrift JDBC/ODBC Server&rdquo;. And &ldquo;Thrift JDBC/ODBC Server&rdquo; won&rsquo;t see your temporary view in &ldquo;Zeppelin&rdquo;.</p>\n<p>To expose a Temporary View via JDBC, you want to create your temporary view and run the HiveThriftServer2 in the same Spark application. In this example, we will do so in the &ldquo;Zeppelin&rdquo; Spark application.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1534343572583_-590101827","id":"20171117-013737_678881016","dateCreated":"2018-08-15T14:32:52+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:420"},{"text":"%md\n# Setup needed for this example\n\nThere is some setup needed for this example.  Please do these steps...\n\n+ Write down the public IP address of the BDCS-CE instance.\n+ Create a BDCS-CE Access Rule that opens up port 10032 to the public internet\n+ Go to Settings..Notebook..Spark2 and add this property (and save it):\nspark.sql.hive.thriftServer.singleSession = true\n","dateUpdated":"2018-08-15T14:32:52+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Setup needed for this example</h1>\n<p>There is some setup needed for this example. Please do these steps&hellip;</p>\n<ul>\n  <li>Write down the public IP address of the BDCS-CE instance.</li>\n  <li>Create a BDCS-CE Access Rule that opens up port 10032 to the public internet</li>\n  <li>Go to Settings..Notebook..Spark2 and add this property (and save it):<br/>spark.sql.hive.thriftServer.singleSession = true</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1534343572583_-590101827","id":"20171117-010043_149885568","dateCreated":"2018-08-15T14:32:52+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:421"},{"text":"%md\n# Steps to run this example\n\n+ Be sure you completed the tutorials mentioned at the top of this note.  This example assumes they have been run successfully.\n+ Run the paragraph to set the OEHCS and Object Store parameters\n+ Run the Streaming Consumer paragraph\n+ Run the Producer paragraph\n+ Run the Start the HiveThriftServer2 paragraph\n+ Connect via JDBC using beeline or DVD\n+ If needed, re-run the Producer paragraph to feed more data\n+ When you are ready to stop, run the \"Stop the running Spark Streaming Consumer\" paragraph\n+ Optionally, restart the notebook to stop the HiveThriftServer2 and get a fresh Spark Zeppelin application","dateUpdated":"2018-08-15T14:32:52+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Steps to run this example</h1>\n<ul>\n  <li>Be sure you completed the tutorials mentioned at the top of this note. This example assumes they have been run successfully.</li>\n  <li>Run the paragraph to set the OEHCS and Object Store parameters</li>\n  <li>Run the Streaming Consumer paragraph</li>\n  <li>Run the Producer paragraph</li>\n  <li>Run the Start the HiveThriftServer2 paragraph</li>\n  <li>Connect via JDBC using beeline or DVD</li>\n  <li>If needed, re-run the Producer paragraph to feed more data</li>\n  <li>When you are ready to stop, run the &ldquo;Stop the running Spark Streaming Consumer&rdquo; paragraph</li>\n  <li>Optionally, restart the notebook to stop the HiveThriftServer2 and get a fresh Spark Zeppelin application</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1534343572584_-592025572","id":"20171117-010326_1017412653","dateCreated":"2018-08-15T14:32:52+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:422"},{"title":"Set OEHCS and Object Store Parameters. Then run this paragraph.","text":"%spark\nz.angularBind(\"BIND_ObjectStorage_Container\", \"journeyC\")\nz.angularBind(\"BIND_OEHCS_ConnectionDescriptor\", z.input(\"OEHCS_ConnectionDescriptor\",\"141.144.144.128:6667\"))\nz.angularBind(\"BIND_OEHCS_Topic\", z.input(\"OEHCS_Topic\",\"gse00010212-TutorialOEHCS\"))\n\n//save these for pyspark\nz.put(\"BIND_OEHCS_Topic\", z.angular(\"BIND_OEHCS_Topic\"))\nz.put(\"BIND_OEHCS_ConnectionDescriptor\", z.angular(\"BIND_OEHCS_ConnectionDescriptor\"))\n\n//save these for shell\nscala.tools.nsc.io.File(\"/var/lib/zeppelin/oehcs.sh\").writeAll(\n  \"export ObjectStorage_Container=\\\"\"+z.angular(\"BIND_ObjectStorage_Container\")+\"\\\"\\n\" +\n  \"export OEHCS_ConnectionDescriptor=\\\"\"+z.angular(\"BIND_OEHCS_ConnectionDescriptor\")+\"\\\"\\n\" +\n  \"export OEHCS_Topic=\\\"\"+z.angular(\"BIND_OEHCS_Topic\")+\"\\\"\\n\"\n)\nprintln(\"done\")\n","dateUpdated":"2018-08-15T14:32:52+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{"OEHCS_Topic":"gse00002281-TutorialOEHCS","ObjectStorage_Container":"citibike","OEHCS_ConnectionDescriptor":"140.86.32.89:6667"},"forms":{"OEHCS_Topic":{"name":"OEHCS_Topic","displayName":"OEHCS_Topic","type":"input","defaultValue":"gse00010212-TutorialOEHCS","hidden":false,"$$hashKey":"object:1241"},"OEHCS_ConnectionDescriptor":{"name":"OEHCS_ConnectionDescriptor","displayName":"OEHCS_ConnectionDescriptor","type":"input","defaultValue":"141.144.144.128:6667","hidden":false,"$$hashKey":"object:1240"}}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"done\n"}]},"apps":[],"jobName":"paragraph_1534343572584_-592025572","id":"20170427-091556_1383806412","dateCreated":"2018-08-15T14:32:52+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:423"},{"title":"Streaming Consumer that defines a Spark SQL Table and writes to Object Store","text":"%spark\n\n{\n    \n\n import _root_.kafka.serializer.StringDecoder //http://stackoverflow.com/questions/36397688/sbt-cannot-import-kafka-encoder-decoder-classes\n import org.apache.spark.streaming._\n import org.apache.spark.streaming.kafka._\n\n val filebase = \"oci://test01bdc01@orasejapan/citibike/streaming\"\n\n\n println(\"Defining a placeholder empty dataframe for our SQL table structure when we don't have any realtime data\")\n\n //use a sample record to capture the column names and types\n val dummyrec=\"{'Birth Year': '1981', 'Stop Time': '2016-12-01 06:03:36', 'End Station Longitude': '-73.99061728', 'Trip Duration': '214', 'Start Station ID': '447', 'Start Station Longitude': '-73.9851615', 'End Station Latitude': '40.76669671', 'End Station Name': 'W 53 St & 10 Ave', 'Start Time': '2016-12-01 06:00:01', 'Start Station Latitude': '40.76370739', 'End Station ID': '480', 'Bike ID': '16669', 'User Type': 'Subscriber', 'Gender': '1', 'Start Station Name': '8 Ave & W 52 St'}\"\n val dummyRDD = sc.parallelize(dummyrec :: Nil)\n var dummyDf=sqlContext.read.json(dummyRDD)\n //df.printSchema()\n\n//Take out spaces in column names because some BI tools cant handle spaces\nvar renamedDummyDf=dummyDf.withColumnRenamed(\"Birth Year\",\"BirthYear\").withColumnRenamed(\"Stop Time\",\"StopTime\").withColumnRenamed(\"End Station Longitude\",\"EndStationLongitude\").withColumnRenamed(\"Trip Duration\",\"TripDuration\").withColumnRenamed(\"Start Station ID\",\"StartStationID\").withColumnRenamed(\"Start Station Longitude\",\"StartStationLongitude\").withColumnRenamed(\"End Station Latitude\",\"EndStationLatitude\").withColumnRenamed(\"End Station Name\",\"EndStationName\").withColumnRenamed(\"Start Time\",\"StartTime\").withColumnRenamed(\"Start Station Latitude\",\"StartStationLatitude\").withColumnRenamed(\"End Station ID\",\"EndStationID\").withColumnRenamed(\"Bike ID\",\"BikeID\").withColumnRenamed(\"User Type\",\"UserType\").withColumnRenamed(\"Start Station Name\",\"StartStationName\")\n\n //create a new DF with zero records (but with same column names/types)\n var emptyDF=renamedDummyDf.filter(\"Gender = 'xxx'\")\n emptyDF.printSchema()\n  \n  \n println(\"Creating new Streaming Context\")\n val ssc = new StreamingContext(sc, Seconds(30))\n \n\n val topic = z.angular(\"BIND_OEHCS_Topic\").toString\n println(\"topic:\"+topic)\n val topicsSet = topic.split(\",\").toSet\n \n val brokers=z.angular(\"BIND_OEHCS_ConnectionDescriptor\").toString\n println(\"brokers:\"+brokers)\n val kafkaParams = Map[String, String](\"metadata.broker.list\" -> brokers)\n\n\n println(\"Creating Kafka DStream\")\n //https://spark.apache.org/docs/1.6.1/streaming-kafka-integration.html\n val messages = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\n      ssc, kafkaParams, topicsSet)\n      \n\n println(\"Setting up operations on DStream\")    \n \n //for debugging, you can print the full contents of the first 10 rows of each batch of messages by uncommenting the following\n //messages.print()\n \n messages.foreachRDD(rdd => {\n     //our Kafka data comes in Key,Value format.  we only care about the value, so use a map to extract just the 2nd element\n     var values=rdd.map(kv => kv._2)\n     \n     //for this example, the value is a JSON string.  Let's make a DataFrame out of the JSON strings\n     var df=sqlContext.read.json(values)\n     \n     var reccount = df.count()\n     //let's print out the count\n     //printf(\"count = %s \\n\",reccount)  This will get printed to the Spark log files, not zeppelin\n     \n     //check to see if we have any rows...\n     if (reccount >0) {\n        //let's print the first row\n        //println(\"first row \",df.first()) This will get printed to the Spark log files, not zeppelin\n        \n        var renamedDf=df.withColumnRenamed(\"Birth Year\",\"BirthYear\").withColumnRenamed(\"Stop Time\",\"StopTime\").withColumnRenamed(\"End Station Longitude\",\"EndStationLongitude\").withColumnRenamed(\"Trip Duration\",\"TripDuration\").withColumnRenamed(\"Start Station ID\",\"StartStationID\").withColumnRenamed(\"Start Station Longitude\",\"StartStationLongitude\").withColumnRenamed(\"End Station Latitude\",\"EndStationLatitude\").withColumnRenamed(\"End Station Name\",\"EndStationName\").withColumnRenamed(\"Start Time\",\"StartTime\").withColumnRenamed(\"Start Station Latitude\",\"StartStationLatitude\").withColumnRenamed(\"End Station ID\",\"EndStationID\").withColumnRenamed(\"Bike ID\",\"BikeID\").withColumnRenamed(\"User Type\",\"UserType\").withColumnRenamed(\"Start Station Name\",\"StartStationName\")\n\n        //let's define a temptable\n        renamedDf.createOrReplaceTempView(\"realtime_bike_trips\")\n        \n        //let's also write this DF to Object Store...\n\n        // save in json format.  the repartition(1) ensures that we write a single output file, which makes sense since we know the output is small\n        //df.repartition(1).write.format(\"json\").mode(\"append\").save(filebase)\n\n\n     } else {\n         //no records.\n         //let's register a df with no data\n         emptyDF.createOrReplaceTempView(\"realtime_bike_trips\")\n     }\n     \n     \n })\n \n println(\"Starting Streaming Context\")\n ssc.start()\n\n println(\"Note: you will need to manually stop this StreamingContext or it will continue forever.  To do so, run: StreamingContext.getActive().map(_.stop(false,true))\")\n println(\"      There is a sample paragraph below that shows you how to do this.\")\n\n println(\"Start of the Consumer done. Go ahead and start the producer if you have not already. With both the Consumer and Producer running, run the Spark SQL queries below and you should see different data every 30 seconds \")\n\n}","dateUpdated":"2018-08-15T14:32:52+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{"0":{"graph":{"mode":"table","height":394.65,"optionOpen":false}}},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1534343572584_-592025572","id":"20170421-100601_289995941","dateCreated":"2018-08-15T14:32:52+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:424"},{"title":"Producer for the more complex example","text":"%sh\n#The arguments are datafile TimeSpeedupFactor RecordsToProduce StartDate StartTime  \n\n# for the more complex example, we are running a timespeedup factor of 5 to push data faster through the system\n\n/u01/bdcsce/usr/local/bin/python ./tutorial_kafka.py citibike/201612-citibike-tripdata.csv 5 300 2016-12-01 06:00:00 2>&1\n","dateUpdated":"2018-08-15T14:59:15+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"false","language":"sh"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1534343572585_-592410321","id":"20170421-103000_112276404","dateCreated":"2018-08-15T14:32:52+0000","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:425","user":"anonymous","dateFinished":"2018-08-15T14:59:12+0000","dateStarted":"2018-08-15T14:59:12+0000"},{"title":"Code to start the HiveThriftSpark2","text":"%spark\n\n//be careful, running this twice will cause your Spark Session to stop.  Fix=go to Settings...Notebook and click Restart\nimport org.apache.spark.sql.hive.HiveContext\nimport org.apache.spark.sql.hive.thriftserver.HiveThriftServer2\nimport org.apache.spark.sql.hive.thriftserver._\n\nval sql = new HiveContext(sc)\nsql.setConf(\"hive.server2.thrift.port\", \"10032\")\nsql.setConf(\"hive.server2.transport.mode\", \"binary\")\nsql.setConf(\"hive.server2.thrift.bind.host\",\"\")\n//sql.setConf(\"spark.sql.hive.thriftServer.singleSession\",\"true\") --this must be set before the session starts (can use zeppelin interpreter properties)\nHiveThriftServer2.startWithContext(sql)","dateUpdated":"2018-08-15T14:32:52+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1534343572585_-592410321","id":"20171116-205659_831316546","dateCreated":"2018-08-15T14:32:52+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:426"},{"text":"%md\n# Test that JDBC works...\n\n## Option 1 - run beeline via SSH\n\n+ Connect via SSH to BDCS-CE\n+ type: beeline\n+ type: !connect jdbc:hive2://127.0.0.1:10032/default\n+ type: show tables;\n+ type: select * from realtime_bike_trips;\n+ type: !exit\n\n## Option 2 - connect via DVD\n\n+ Make a connection of type \"Spark\"\n\n - For the hostname, you can use the Public IP of BDCS-CE (assuming you created an Access Rule for the internet for port 10032).  Or you can use 127.0.0.1 (assuming you created a SSH tunnel for port 10032)\n - Use 10032 as port\n - Use \"hive\" as username\n - Use \"x\" as password\n\n+ Create a Data Set using your new connection\n\n- You may not see any tables with DVD3/4.  In that case, click on Enter SQL and choose \"select * from realtime_bike_trips\".\n- Set the Refresh property to \"Live\"\n\n\n","dateUpdated":"2018-08-15T14:32:52+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Test that JDBC works&hellip;</h1>\n<h2>Option 1 - run beeline via SSH</h2>\n<ul>\n  <li>Connect via SSH to BDCS-CE</li>\n  <li>type: beeline</li>\n  <li>type: !connect jdbc:hive2://127.0.0.1:10032/default</li>\n  <li>type: show tables;</li>\n  <li>type: select * from realtime_bike_trips;</li>\n  <li>type: !exit</li>\n</ul>\n<h2>Option 2 - connect via DVD</h2>\n<ul>\n  <li>\n  <p>Make a connection of type &ldquo;Spark&rdquo;</p></li>\n  <li>\n  <p>For the hostname, you can use the Public IP of BDCS-CE (assuming you created an Access Rule for the internet for port 10032). Or you can use 127.0.0.1 (assuming you created a SSH tunnel for port 10032)</p></li>\n  <li>Use 10032 as port</li>\n  <li>Use &ldquo;hive&rdquo; as username</li>\n  <li>Use &ldquo;x&rdquo; as password</li>\n  <li>\n  <p>Create a Data Set using your new connection</p></li>\n  <li>\n  <p>You may not see any tables with DVD3/4. In that case, click on Enter SQL and choose &ldquo;select * from realtime_bike_trips&rdquo;.</p></li>\n  <li>Set the Refresh property to &ldquo;Live&rdquo;</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1534343572585_-592410321","id":"20171117-011610_1778274376","dateCreated":"2018-08-15T14:32:52+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:427"},{"title":"Spark SQL to query realtime_bike_trips (use this as a sanity check.  This does not use JDBC)","text":"%sql\nselect * from realtime_bike_trips","dateUpdated":"2018-08-15T14:32:52+0000","config":{"editorSetting":{"language":"sql"},"colWidth":12,"editorMode":"ace/mode/sql","title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"Bike ID","index":0,"aggr":"sum"}],"values":[{"name":"Birth Year","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"Bike ID","index":0,"aggr":"sum"},"yAxis":{"name":"Birth Year","index":1,"aggr":"sum"}}}}],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1534343572586_-591256074","id":"20170421-094901_1826207284","dateCreated":"2018-08-15T14:32:52+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:428"},{"title":"Spark code to stop the running Spark Streaming consumer ","text":"%spark\n{\n    import org.apache.spark.streaming._\n    println(\"Stopping any active StreamingContext.  May take a minute.\")\n    StreamingContext.getActive().map(_.stop(false,true))\n    println(\"done\")\n}","dateUpdated":"2018-08-15T14:32:52+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Stopping any active StreamingContext.  May take a minute.\ndone\n"}]},"apps":[],"jobName":"paragraph_1534343572586_-591256074","id":"20170419-115308_1919654685","dateCreated":"2018-08-15T14:32:52+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:429"},{"title":"Script to look at last lines of Spark Zeppelin log file","text":"%sh\ntail -250 /u01/bdcsce/data/var/log/zeppelin/zeppelin-interpreter-spark2-spark-zeppelin-dcbnov15c-bdcsce-1.log","dateUpdated":"2018-08-15T14:32:52+0000","config":{"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":"false"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1534343572586_-591256074","id":"20171117-010910_1338221768","dateCreated":"2018-08-15T14:32:52+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:430"},{"text":"%md\n### Change Log\nAugust 15, 2018 - Switched to Python2.7\nNovember 16, 2017 - Created for 17.4.1\n","dateUpdated":"2018-08-15T14:59:40+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Change Log</h3>\n<p>August 15, 2018 - Switched to Python2.7<br/>November 16, 2017 - Created for 17.4.1</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1534343572586_-591256074","id":"20170419-121108_1129923637","dateCreated":"2018-08-15T14:32:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:431","user":"anonymous","dateFinished":"2018-08-15T14:59:40+0000","dateStarted":"2018-08-15T14:59:40+0000"},{"text":"%md\n","dateUpdated":"2018-08-15T14:32:52+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":"true"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1534343572587_-591640823","id":"20170728-144609_433937768","dateCreated":"2018-08-15T14:32:52+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:432"}],"name":"Journeys/New Data Lake/Streaming/Advanced/Querying Temporary View via JDBC","id":"2DMPN1KG4","angularObjects":{"2DNN4423S:shared_process":[],"2DMG17QTJ:shared_process":[],"2DQXUFTTR:shared_process":[],"2DP58Y9WG:shared_process":[],"2C4U48MY3_spark2:shared_process":[],"2DPSPNKH1:shared_process":[],"2DQTY9CS9:shared_process":[],"2DPZR9NJS:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}