{"paragraphs":[{"text":"%md\n# Tutorial: Working with Oracle Autonomous Data Warehouse Cloud\n\nThis tutorial was built for BDC version 18.1.6 as part of the New Data Lake User Journey.  Questions and feedback about the tutorial: <david.bayard@oracle.com>\n\nThe Autonomous Data Warehouse Cloud provides an easy-to-use, fully autonomous database that scales elastically, delivers fast query performance and requires no database administration.  It is a fully-managed data warehouse designed to support all standard SQL and business intelligence (BI) tools and deliver scalable analytic query performance.  Find out more at the <a href=\"https://cloud.oracle.com/en_US/datawarehouse\" target=\"_blank\">ADWC website</a> or check out the <a href=\"https://docs.oracle.com/en/cloud/paas/autonomous-data-warehouse-cloud/index.html\" target=\"_blank\">product documentation.</a>\n\n\nNote: This tutorial replaces the Working with Oracle Database tutorial, in that this tutorial goes the extra-step of setting up the 18.1 JDBC-OCI connectivity.  You can use this same connectivity to other non-ADWC Oracle databases.  In other words, if you follow this tutorial, don't follow the steps in the Working with Oracle Database tutorial as that tutorial assumes you want to use the 12.1 JDBC driver (not the 18.1 JDBC driver).\n\n","user":"anonymous","dateUpdated":"2018-03-23T14:13:36+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Tutorial: Working with Oracle Autonomous Data Warehouse Cloud</h1>\n<p>This tutorial was built for BDC version 18.1.6 as part of the New Data Lake User Journey. Questions and feedback about the tutorial: <a href=\"mailto:&#x64;&#x61;&#118;&#105;d&#x2e;&#x62;a&#121;&#x61;&#x72;&#100;&#64;&#111;&#114;&#97;&#x63;&#x6c;&#101;&#46;&#99;&#111;&#x6d;\">&#x64;&#x61;&#118;&#105;d&#x2e;&#x62;a&#121;&#x61;&#x72;&#100;&#64;&#111;&#114;&#97;&#x63;&#x6c;&#101;&#46;&#99;&#111;&#x6d;</a></p>\n<p>The Autonomous Data Warehouse Cloud provides an easy-to-use, fully autonomous database that scales elastically, delivers fast query performance and requires no database administration. It is a fully-managed data warehouse designed to support all standard SQL and business intelligence (BI) tools and deliver scalable analytic query performance. Find out more at the <a href=\"https://cloud.oracle.com/en_US/datawarehouse\" target=\"_blank\">ADWC website</a> or check out the <a href=\"https://docs.oracle.com/en/cloud/paas/autonomous-data-warehouse-cloud/index.html\" target=\"_blank\">product documentation.</a></p>\n<p>Note: This tutorial replaces the Working with Oracle Database tutorial, in that this tutorial goes the extra-step of setting up the 18.1 JDBC-OCI connectivity. You can use this same connectivity to other non-ADWC Oracle databases. In other words, if you follow this tutorial, don&rsquo;t follow the steps in the Working with Oracle Database tutorial as that tutorial assumes you want to use the 12.1 JDBC driver (not the 18.1 JDBC driver).</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1521658284124_-1556838399","id":"20170414-115315_151675592","dateCreated":"2018-03-21T18:51:24+0000","dateStarted":"2018-03-23T14:13:36+0000","dateFinished":"2018-03-23T14:13:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:530"},{"text":"%md\n# About connecting to ADWC\n\nConnections to the Autonomous Data Warehouse Cloud are made over the public Internet, and all applications must use a secure connection to the Autonomous Data Warehouse Cloud. If you are familiar with using an Oracle Database within your own data center, you may not have previously used these kind of secure SQL*Net connections, so some of the connection details may be new to you.\n\nConnections to Autonomous Data Warehouse Cloud use certificate authentication and Secure Sockets Layer (SSL). This ensures that there is no unauthorized access to the Autonomous Data Warehouse Cloud and that communications between the client and server are fully encrypted and cannot be intercepted or altered. To enable these features, it requires that the client machine has a copy of a special set of files (known as the \"Client Credentials\" or the \"wallet\") that are configured for your ADWC instance.\n\nFor this tutorial, we will use the 18.1 JDBC Thin driver as JDBC 18.1 makes working with wallets easier than earlier versions.\n\nTo use 18.1 JDBC Thin with BDC, here are the high-level steps we need to follow:\n\n- Copy the Client Credentials zip file from your ADWC instance to BDC\n\n- Install the 18.1 JDBC Thin jar files on the BDC servers\n\n- Configure the Spark environment to work with 18.1 JDBC libraries\n\n- Configure the Zeppelin environment to use the 18.1 Oracle JDBC driver for the JDBC interpreter and Spark interpreter\n\n\nLearn more about ADWC Connectivity in the <a href=\"https://docs.oracle.com/en/cloud/paas/autonomous-data-warehouse-cloud/user/connect-data-warehouse.html\" target=\"_blank\">product documentation</a>.\n","user":"anonymous","dateUpdated":"2018-03-23T14:23:36+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>About connecting to ADWC</h1>\n<p>Connections to the Autonomous Data Warehouse Cloud are made over the public Internet, and all applications must use a secure connection to the Autonomous Data Warehouse Cloud. If you are familiar with using an Oracle Database within your own data center, you may not have previously used these kind of secure SQL*Net connections, so some of the connection details may be new to you.</p>\n<p>Connections to Autonomous Data Warehouse Cloud use certificate authentication and Secure Sockets Layer (SSL). This ensures that there is no unauthorized access to the Autonomous Data Warehouse Cloud and that communications between the client and server are fully encrypted and cannot be intercepted or altered. To enable these features, it requires that the client machine has a copy of a special set of files (known as the &ldquo;Client Credentials&rdquo; or the &ldquo;wallet&rdquo;) that are configured for your ADWC instance.</p>\n<p>For this tutorial, we will use the 18.1 JDBC Thin driver as JDBC 18.1 makes working with wallets easier than earlier versions.</p>\n<p>To use 18.1 JDBC Thin with BDC, here are the high-level steps we need to follow:</p>\n<ul>\n  <li>\n  <p>Copy the Client Credentials zip file from your ADWC instance to BDC</p></li>\n  <li>\n  <p>Install the 18.1 JDBC Thin jar files on the BDC servers</p></li>\n  <li>\n  <p>Configure the Spark environment to work with 18.1 JDBC libraries</p></li>\n  <li>\n  <p>Configure the Zeppelin environment to use the 18.1 Oracle JDBC driver for the JDBC interpreter and Spark interpreter</p></li>\n</ul>\n<p>Learn more about ADWC Connectivity in the <a href=\"https://docs.oracle.com/en/cloud/paas/autonomous-data-warehouse-cloud/user/connect-data-warehouse.html\" target=\"_blank\">product documentation</a>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1521658284126_-1556068901","id":"20180105-145319_2025443570","dateCreated":"2018-03-21T18:51:24+0000","dateStarted":"2018-03-23T14:23:36+0000","dateFinished":"2018-03-23T14:23:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:531"},{"text":"%md\n# Copy the Client Credentials zip file from your ADWC instance to BDC\n\n## Download the Client Credentials zip file from your ADWC instance\n\n\nTo download client credentials, do the following:\n\n- Navigate to the Service Console for Autonomous Data Warehouse Cloud.\n- Choose Administration.\n- On the Administration page Choose Download Client Credentials.\n- On the Client Credentials dialog, enter a wallet password and confirm the password.\n- Click Download to save the client security credentials zip file.\n\n## Upload the Client Credentials zip file to your BDC instance\n\nFor simplicity, we will upload the Client Credentials using the BDC Console.  Follow these steps:\n\n- Navigate to Data Stores in the Big Data Cloud console\n- Click on HDFS\n- Click on the tmp folder\n- Click on the Upload button\n- Select the Client Credentials zip file (it will likely be named something like wallet_DBNAME.zip) and upload it\n\nAt this point, you should see a wallet*.zip file in the HDFS /tmp directory.\n\n## Move the Client Credentials zip file from HDFS to the linux file system\n\nFinally, we need to move the client credentials zip file to the linux file system and unzip it.\n\n- Run the following paragraph to move and unzip the client credentials zip file\n\n","user":"anonymous","dateUpdated":"2018-03-21T23:53:49+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Copy the Client Credentials zip file from your ADWC instance to BDC</h1>\n<h2>Download the Client Credentials zip file from your ADWC instance</h2>\n<p>To download client credentials, do the following:</p>\n<ul>\n  <li>Navigate to the Service Console for Autonomous Data Warehouse Cloud.</li>\n  <li>Choose Administration.</li>\n  <li>On the Administration page Choose Download Client Credentials.</li>\n  <li>On the Client Credentials dialog, enter a wallet password and confirm the password.</li>\n  <li>Click Download to save the client security credentials zip file.</li>\n</ul>\n<h2>Upload the Client Credentials zip file to your BDC instance</h2>\n<p>For simplicity, we will upload the Client Credentials using the BDC Console. Follow these steps:</p>\n<ul>\n  <li>Navigate to Data Stores in the Big Data Cloud console</li>\n  <li>Click on HDFS</li>\n  <li>Click on the tmp folder</li>\n  <li>Click on the Upload button</li>\n  <li>Select the Client Credentials zip file (it will likely be named something like wallet_DBNAME.zip) and upload it</li>\n</ul>\n<p>At this point, you should see a wallet*.zip file in the HDFS /tmp directory.</p>\n<h2>Move the Client Credentials zip file from HDFS to the linux file system</h2>\n<p>Finally, we need to move the client credentials zip file to the linux file system and unzip it.</p>\n<ul>\n  <li>Run the following paragraph to move and unzip the client credentials zip file</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1521658284126_-1556068901","id":"20180105-150920_1546634221","dateCreated":"2018-03-21T18:51:24+0000","dateStarted":"2018-03-21T23:53:49+0000","dateFinished":"2018-03-21T23:53:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:532"},{"title":"Script to move the Client Credentials zip to the linux file system","text":"%sh\nsudo mkdir /opt/oracle/dbconnector/tns_admin\nsudo chown zeppelin /opt/oracle/dbconnector/tns_admin\ncd /opt/oracle/dbconnector/tns_admin\nrm *\nhadoop fs -ls /tmp\nhadoop fs -get \"/tmp/wallet*.zip\" wallet.zip\nunzip -u wallet.zip\n\n#update the wallet directory path in the sqlnet.ora\ncp sqlnet.ora sqlnet.ora.save\ncat <<EOF > sqlnet.ora\nWALLET_LOCATION = (SOURCE = (METHOD = file) (METHOD_DATA = (DIRECTORY=\"/opt/oracle/dbconnector/tns_admin\")))\nSSL_SERVER_DN_MATCH=yes\nEOF\n\nls\necho \"done\"\n","user":"anonymous","dateUpdated":"2018-03-22T12:57:51+0000","config":{"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":"false"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521658284127_-1556453650","id":"20180105-152947_188131871","dateCreated":"2018-03-21T18:51:24+0000","dateStarted":"2018-03-22T12:57:39+0000","dateFinished":"2018-03-22T12:57:44+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:533"},{"text":"%md\n# Install the Oracle Instant Client (already installed, just need to test it)\n\nWith BDC, the Oracle Instant Client should already be installed.  You should see it at /opt/oracle/dbconnector/instantclient.  As of 17.4.6, the Oracle Instant Client version is 12.2.0.1.\n\nYou can run the next paragraph to test connectivity with SQL*Plus.\n\n","dateUpdated":"2018-03-21T18:51:24+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Install the Oracle Instant Client (already installed, just need to test it)</h1>\n<p>With BDC, the Oracle Instant Client should already be installed. You should see it at /opt/oracle/dbconnector/instantclient. As of 17.4.6, the Oracle Instant Client version is 12.2.0.1.</p>\n<p>You can run the next paragraph to test connectivity with SQL*Plus.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1521658284127_-1556453650","id":"20180104-163126_74184324","dateCreated":"2018-03-21T18:51:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:534"},{"title":"Script to test ADWC Connectivity with SQL*Plus (you will need to edit the username/password and tns alias)","text":"%sh\nexport TNS_ADMIN=/opt/oracle/dbconnector/tns_admin\nexport LD_LIBRARY_PATH=/opt/oracle/dbconnector/instantclient:$LD_LIBRARY_PATH\nexport PATH=/opt/oracle/dbconnector/instantclient:$PATH\n\n\nsqlplus 'dave/Welcome1Welcome1@DAVEDW2_high'\n","user":"anonymous","dateUpdated":"2018-03-23T13:42:26+0000","config":{"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":"false"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521658284127_-1556453650","id":"20180104-163513_422676257","dateCreated":"2018-03-21T18:51:24+0000","dateStarted":"2018-03-23T13:42:23+0000","dateFinished":"2018-03-23T13:42:23+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:535"},{"text":"%md\n# Copy the 18.1 JDBC Drivers to BDC\n\nDownload from http://den02sxd.us.oracle.com/18100_RELEASE/\n\nCopy these 4 files to the HDFS tmp directory: ojdbc8.jar, oraclepki.jar, osdt_cert.jar, osdt_core.jar\n\nThen run the following paragraph to put them in place\n\n","user":"anonymous","dateUpdated":"2018-03-22T13:08:49+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":"true"},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Copy the 18.1 JDBC Drivers to BDC</h1>\n<p>Download from <a href=\"http://den02sxd.us.oracle.com/18100_RELEASE/\">http://den02sxd.us.oracle.com/18100_RELEASE/</a></p>\n<p>Copy these 4 files to the HDFS tmp directory: ojdbc8.jar, oraclepki.jar, osdt_cert.jar, osdt_core.jar</p>\n<p>Then run the following paragraph to put them in place</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1521723640484_1994838285","id":"20180322-130040_824116071","dateCreated":"2018-03-22T13:00:40+0000","dateStarted":"2018-03-22T13:08:49+0000","dateFinished":"2018-03-22T13:08:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:536"},{"title":"Script to put 18.1 jdbc files in place","text":"%sh\nsudo mkdir /opt/oracle/dbconnector/jdbc18_1\nsudo chown zeppelin /opt/oracle/dbconnector/jdbc18_1\ncd /opt/oracle/dbconnector/jdbc18_1\nrm *\nhadoop fs -ls /tmp\nhadoop fs -get \"/tmp/ojdbc8.jar\" ojdbc8.jar\nhadoop fs -get \"/tmp/oraclepki.jar\" oraclepki.jar\nhadoop fs -get \"/tmp/osdt_cert.jar\" osdt_cert.jar\nhadoop fs -get \"/tmp/osdt_core.jar\" osdt_core.jar\n\nls -l\n\n\necho \"done\"\n\n","user":"anonymous","dateUpdated":"2018-03-22T13:07:47+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":"false"},"editorMode":"ace/mode/sh","editorHide":false,"tableHide":false,"title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521723846266_1010774836","id":"20180322-130406_38712429","dateCreated":"2018-03-22T13:04:06+0000","dateStarted":"2018-03-22T13:06:56+0000","dateFinished":"2018-03-22T13:07:09+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:537"},{"text":"%md\n# Configure the Zeppelin JDBC Interpreter\n\nFollow these steps:\n\n- In the BDC console, click on Settings\n- Then click on Notebook\n- Scroll down to the JDBC interpreter section and click the Edit button\n- In the properties section, add the following properties\n    - adwc-sh.driver = oracle.jdbc.OracleDriver \n    - adwc-sh.user = sh\n    - adwc-sh.url = jdbc:oracle:thin:@DAVEDW2_low?TNS_ADMIN=/opt/oracle/dbconnector/tns_admin\n    - adwc-sh.password = YourPassword\n    - Update values like \"sh\" with your database username.  Update the last part of the url with your tns alias.  Update password with your password.\n- In the dependencies section, overwrite/replace the ojdbc7 dependency with this value:\n/opt/oracle/dbconnector/jdbc18_1/ojdbc8.jar\n- Then add 3 more dependency lines for:\n/opt/oracle/dbconnector/jdbc18_1/oraclepki.jar\n/opt/oracle/dbconnector/jdbc18_1/osdt_cert.jar\n/opt/oracle/dbconnector/jdbc18_1/osdt_core.jar\n- Click Save\n- Then click OK to restart the JDBC interpreter\n\n","user":"anonymous","dateUpdated":"2018-03-22T13:48:24+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Configure the Zeppelin JDBC Interpreter</h1>\n<p>Follow these steps:</p>\n<ul>\n  <li>In the BDC console, click on Settings</li>\n  <li>Then click on Notebook</li>\n  <li>Scroll down to the JDBC interpreter section and click the Edit button</li>\n  <li>In the properties section, add the following properties\n    <ul>\n      <li>adwc-sh.driver = oracle.jdbc.OracleDriver</li>\n      <li>adwc-sh.user = sh</li>\n      <li>adwc-sh.url = jdbc:oracle:thin:@DAVEDW2_low?TNS_ADMIN=/opt/oracle/dbconnector/tns_admin</li>\n      <li>adwc-sh.password = YourPassword</li>\n      <li>Update values like &ldquo;sh&rdquo; with your database username. Update the last part of the url with your tns alias. Update password with your password.</li>\n    </ul>\n  </li>\n  <li>In the dependencies section, overwrite/replace the ojdbc7 dependency with this value:<br/>/opt/oracle/dbconnector/jdbc18_1/ojdbc8.jar</li>\n  <li>Then add 3 more dependency lines for:<br/>/opt/oracle/dbconnector/jdbc18_1/oraclepki.jar<br/>/opt/oracle/dbconnector/jdbc18_1/osdt_cert.jar<br/>/opt/oracle/dbconnector/jdbc18_1/osdt_core.jar</li>\n  <li>Click Save</li>\n  <li>Then click OK to restart the JDBC interpreter</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1521658284129_-1571074108","id":"20180105-155244_1736223466","dateCreated":"2018-03-21T18:51:24+0000","dateStarted":"2018-03-22T13:48:24+0000","dateFinished":"2018-03-22T13:48:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:538"},{"title":"Script to patch Zeppelin to fix performance issue (ZEPPELIN-1962)","text":"%sh\n# There are some performance issues with Zeppelin 0.7.x jdbc interpreter and OracleDB due to a costly (slow) autocompletion feature.\n# See https://issues.apache.org/jira/browse/ZEPPELIN-1962  (also being tracked as oracle bug 27037280)\n# The workaround for Zeppelin is 0.7.x is to use a patched zeppelin jdbc interpreter jar file which disables autocompletion.\n\nmkdir /tmp/zepjdbc\ncd /tmp/zepjdbc\nwget https://issues.apache.org/jira/secure/attachment/12874854/zeppelin-jdbc-0.7.2.jar\nls -l zepp*\n\necho \"now putting files into place\"\nsudo mv /u01/bdcsce/usr/hdp/2.4.2.0-258/zeppelin-spark21/interpreter/jdbc/zeppelin-jdbc-0.7.0.2.6.0.3-8.jar /u01/bdcsce/usr/hdp/2.4.2.0-258/zeppelin-spark21/interpreter/jdbc/zeppelin-jdbc-0.7.0.2.6.0.3-8.jar.orig\nsudo cp /tmp/zepjdbc/zeppelin-jdbc-0.7.2.jar /u01/bdcsce/usr/hdp/2.4.2.0-258/zeppelin-spark21/interpreter/jdbc/zeppelin-jdbc-0.7.2.jar\n\nhostname -f\necho \"done\"\n\n# You need to restart the zeppelin jdbc interpreter, but the next paragraph will do that for us\n","user":"anonymous","dateUpdated":"2018-03-22T13:22:35+0000","config":{"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":"false"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521658284129_-1571074108","id":"20180105-171405_1432133900","dateCreated":"2018-03-21T18:51:24+0000","dateStarted":"2018-03-22T13:19:42+0000","dateFinished":"2018-03-22T13:19:42+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:539"},{"text":"%md\n# Additional step to correct Password for JDBC Interpreter\n\nThis step is currently required because of our temporary fix to the ZEPPELIN-1962 issue.  Long term, this step will not be needed.\n\n- Copy the hostname of your BDC server.  In the above paragraph, the hostname should be printed out on the next-to-last line of the output.\n- Click on Settings\n- Click on Notebook\n- Scroll down to the JDBC Interpreter section\n- Click on Edit\n- Find the property adwc-sh.jceks.file\n- Edit the value of the property so that instead of \"hdfs\" it now reads \"hdfs@hostname\".  For instance, here is an example after editing:\njceks://hdfs@dcbdec13-bdcsce-1.compute-gse00010212.oraclecloud.internal/user/zeppelin/interpreter-store.jks\n- Click Save\n- Click OK to restart","dateUpdated":"2018-03-21T18:51:24+0000","config":{"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Additional step to correct Password for JDBC Interpreter</h1>\n<p>This step is currently required because of our temporary fix to the ZEPPELIN-1962 issue. Long term, this step will not be needed.</p>\n<ul>\n  <li>Copy the hostname of your BDC server. In the above paragraph, the hostname should be printed out on the next-to-last line of the output.</li>\n  <li>Click on Settings</li>\n  <li>Click on Notebook</li>\n  <li>Scroll down to the JDBC Interpreter section</li>\n  <li>Click on Edit</li>\n  <li>Find the property adwc-sh.jceks.file</li>\n  <li>Edit the value of the property so that instead of &ldquo;hdfs&rdquo; it now reads &quot;<a href=\"mailto:&#104;&#100;f&#115;&#64;&#104;&#x6f;&#x73;&#x74;&#x6e;&#97;&#109;e&#x22;\">&#104;&#100;f&#115;&#64;&#104;&#x6f;&#x73;&#x74;&#x6e;&#97;&#109;e&#x22;</a>. For instance, here is an example after editing:<br/><a href=\"jceks://hdfs@dcbdec13-bdcsce-1.compute-gse00010212.oraclecloud.internal/user/zeppelin/interpreter-store.jks\">jceks://hdfs@dcbdec13-bdcsce-1.compute-gse00010212.oraclecloud.internal/user/zeppelin/interpreter-store.jks</a></li>\n  <li>Click Save</li>\n  <li>Click OK to restart</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1521658284130_-1569919861","id":"20180105-171636_804449940","dateCreated":"2018-03-21T18:51:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:540"},{"title":"Script to test Zeppelin JDBC interpreter","text":"%jdbc(adwc-admin)\nselect user from dual\n","user":"anonymous","dateUpdated":"2018-03-22T13:21:46+0000","config":{"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521658284130_-1569919861","id":"20180105-165650_115558078","dateCreated":"2018-03-21T18:51:24+0000","dateStarted":"2018-03-22T13:21:46+0000","dateFinished":"2018-03-22T13:21:49+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:541"},{"text":"%md\n# Configure the Spark environment to work with the 18.1 JDBC libraries\n\nFirst, connect to Ambari (see the Extra tutorial on Connecting to Ambari if you need help).  Then while in Ambari, do the following:\n\n\n## Setup the SPARK environment\n\n- In Ambari, click on Spark2\n- Click on Configs then Advanced\n- Expand Advanced spark2-env and edit spark_thrift_cmd_opts\nsearch for /opt/oracle/bdcsce/current/lib/ojdbc7.jar. change to /u01/bdcsce/opt/oracle/dbconnector/jdbc18_1/ojdbc8.jar\n\nreplace:    /opt/oracle/bdcsce/current/lib/ojdbc7.jar:\nwith:       /u01/bdcsce/opt/oracle/dbconnector/jdbc18_1/ojdbc8.jar:/u01/bdcsce/opt/oracle/dbconnector/jdbc18_1/oraclepki.jar:/u01/bdcsce/opt/oracle/dbconnector/jdbc18_1/osdt_core.jar:/u01/bdcsce/opt/oracle/dbconnector/jdbc18_1/osdt_cert.jar:\n\nreplace:    /opt/oracle/bdcsce/current/lib/ojdbc7.jar,\nwith:       /u01/bdcsce/opt/oracle/dbconnector/jdbc18_1/ojdbc8.jar,/u01/bdcsce/opt/oracle/dbconnector/jdbc18_1/oraclepki.jar,/u01/bdcsce/opt/oracle/dbconnector/jdbc18_1/osdt_core.jar,/u01/bdcsce/opt/oracle/dbconnector/jdbc18_1/osdt_cert.jar,\nNOTE ojdbc7 appears 3 times. You need to change all 3.  \n\n\n\n## Restart services\n\n- In Ambari, click on the Actions button underneath the list of services\n- Choose Restart All Required\n- Then click on Confirm Restart All\n","user":"anonymous","dateUpdated":"2018-03-23T14:24:25+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Configure the Spark environment to work with the 18.1 JDBC libraries</h1>\n<p>First, connect to Ambari (see the Extra tutorial on Connecting to Ambari if you need help). Then while in Ambari, do the following:</p>\n<h2>Setup the SPARK environment</h2>\n<ul>\n  <li>In Ambari, click on Spark2</li>\n  <li>Click on Configs then Advanced</li>\n  <li>Expand Advanced spark2-env and edit spark_thrift_cmd_opts<br/>search for /opt/oracle/bdcsce/current/lib/ojdbc7.jar. change to /u01/bdcsce/opt/oracle/dbconnector/jdbc18_1/ojdbc8.jar</li>\n</ul>\n<p>replace: /opt/oracle/bdcsce/current/lib/ojdbc7.jar:<br/>with: /u01/bdcsce/opt/oracle/dbconnector/jdbc18_1/ojdbc8.jar:/u01/bdcsce/opt/oracle/dbconnector/jdbc18_1/oraclepki.jar:/u01/bdcsce/opt/oracle/dbconnector/jdbc18_1/osdt_core.jar:/u01/bdcsce/opt/oracle/dbconnector/jdbc18_1/osdt_cert.jar:</p>\n<p>replace: /opt/oracle/bdcsce/current/lib/ojdbc7.jar,<br/>with: /u01/bdcsce/opt/oracle/dbconnector/jdbc18_1/ojdbc8.jar,/u01/bdcsce/opt/oracle/dbconnector/jdbc18_1/oraclepki.jar,/u01/bdcsce/opt/oracle/dbconnector/jdbc18_1/osdt_core.jar,/u01/bdcsce/opt/oracle/dbconnector/jdbc18_1/osdt_cert.jar,<br/>NOTE ojdbc7 appears 3 times. You need to change all 3. </p>\n<h2>Restart services</h2>\n<ul>\n  <li>In Ambari, click on the Actions button underneath the list of services</li>\n  <li>Choose Restart All Required</li>\n  <li>Then click on Confirm Restart All</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1521658284129_-1571074108","id":"20180105-154608_1320202625","dateCreated":"2018-03-21T18:51:24+0000","dateStarted":"2018-03-23T14:24:25+0000","dateFinished":"2018-03-23T14:24:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:542"},{"text":"%md\n# Configure the Zeppelin Spark Interpeter\n\n- In the BDC console, click on Settings\n- Then click on Notebook\n- Scroll down to the spark2 interpreter section and click the Edit button\n- In the dependencies section, overwrite/replace the ojdbc7 dependency with this value:\n/opt/oracle/dbconnector/jdbc18_1/ojdbc8.jar\n- Then add 3 more dependencies for:\n/opt/oracle/dbconnector/jdbc18_1/oraclepki.jar\n/opt/oracle/dbconnector/jdbc18_1/osdt_cert.jar\n/opt/oracle/dbconnector/jdbc18_1/osdt_core.jar\n- Click Save\n- Then click OK to restart the JDBC interpreter\n","user":"anonymous","dateUpdated":"2018-03-22T13:49:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Configure the Zeppelin Spark Interpeter</h1>\n<ul>\n  <li>In the BDC console, click on Settings</li>\n  <li>Then click on Notebook</li>\n  <li>Scroll down to the spark2 interpreter section and click the Edit button</li>\n  <li>In the dependencies section, overwrite/replace the ojdbc7 dependency with this value:<br/>/opt/oracle/dbconnector/jdbc18_1/ojdbc8.jar</li>\n  <li>Then add 3 more dependencies for:<br/>/opt/oracle/dbconnector/jdbc18_1/oraclepki.jar<br/>/opt/oracle/dbconnector/jdbc18_1/osdt_cert.jar<br/>/opt/oracle/dbconnector/jdbc18_1/osdt_core.jar</li>\n  <li>Click Save</li>\n  <li>Then click OK to restart the JDBC interpreter</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1521658284130_-1569919861","id":"20180105-155305_2073848723","dateCreated":"2018-03-21T18:51:24+0000","dateStarted":"2018-03-22T13:49:04+0000","dateFinished":"2018-03-22T13:49:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:543"},{"title":"Script to ensure that 18.1 jdbc driver is used by Spark executors","text":"%sh\nsudo mv /opt/oracle/bdcsce/current/lib/ojdbc7.jar ~zeppelin/ojdbc7.jar.dontuse\n\n#make a small permission fix so that spark can query the local file system for later in this tutorial\nchmod a+rx /var/lib/zeppelin\n\necho \"done\"","user":"anonymous","dateUpdated":"2018-03-23T14:24:42+0000","config":{"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":"false"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521658284131_-1570304610","id":"20180105-175940_1530123000","dateCreated":"2018-03-21T18:51:24+0000","dateStarted":"2018-03-22T13:50:55+0000","dateFinished":"2018-03-22T13:50:56+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:544"},{"title":"Script to test Spark interpreter","text":"%spark\n\n// BEFORE RUNNING THIS, YOU WILL NEED TO EDIT THIS\n//  1.Insert your database Connect String\n//  2.Insert your database user name\n//  3.Insert your database password\n\n\n//define URL for the Oracle JDBC driver\nprintln(\">>>>>>>Defining url for Oracle JDBC\")\n\nval url=\"jdbc:oracle:thin:@DAVEDW2_low?TNS_ADMIN=/opt/oracle/dbconnector/tns_admin\"\n\n//define the username and password as properties\nprintln(\">>>>>>>Defining Oracle JDBC username and password\")\nprintln(url)\nval prop = new java.util.Properties\n\n\nprop.setProperty(\"user\",\"dave\")\nprop.setProperty(\"password\",\"Welcome1Welcome1\")\n\n\nprop.setProperty(\"driver\",\"oracle.jdbc.OracleDriver\") //the driver is needed to be defined with Spark 1.6.1 due to https://issues.apache.org/jira/browse/SPARK-14204\n\n//now you can use JDBC commands like: val movies = sqlContext.read.jdbc(url,\"movie\",prop)\nval utables = sqlContext.read.jdbc(url,\"user_tables\",prop)\n//utables.explain()\nutables.printSchema()\n//utables.show()\n\n//register the emp dataframe as a SparkSQL table\nutables.createOrReplaceTempView(\"utables_sparksql\")\n\n//we can also do specific queries like the following (note that we write our query as if it was a subquery in the FROM section of a select statement)\nval ora_query = sqlContext.read.jdbc(url, \"(select u.tablespace_name, ts.status, count(*) tcount from user_tables u, user_tablespaces ts where u.tablespace_name=ts.tablespace_name group by u.tablespace_name, ts.status) eq\", prop)\nora_query.show()\n//emp_query.explain()\n\nprintln(\"done\")","user":"anonymous","dateUpdated":"2018-04-11T12:26:54+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521658284131_-1570304610","id":"20180105-175253_1061772504","dateCreated":"2018-03-21T18:51:24+0000","dateStarted":"2018-03-23T14:07:12+0000","dateFinished":"2018-03-23T14:07:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:545","errorMessage":""},{"title":"SparkSQL to test connection","text":"%sql\nselect table_name, num_rows, last_analyzed from utables_sparksql\nwhere num_rows > 0\norder by num_rows desc","user":"anonymous","dateUpdated":"2018-03-23T13:43:17+0000","config":{"editorSetting":{"language":"sql"},"colWidth":12,"editorMode":"ace/mode/sql","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521658284131_-1570304610","id":"20180105-180146_1838254935","dateCreated":"2018-03-21T18:51:24+0000","dateStarted":"2018-03-23T13:43:17+0000","dateFinished":"2018-03-23T13:43:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:546","errorMessage":""},{"text":"%md\n# Using Spark to write to the Oracle Database\n\nNow we will show a working example of writing back to the Oracle Database from Spark.  \n\nFor this example, we will use Spark to read in some Citibike data and write that data into an Oracle table.\n","dateUpdated":"2018-03-21T18:51:24+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Using Spark to write to the Oracle Database</h1>\n<p>Now we will show a working example of writing back to the Oracle Database from Spark. </p>\n<p>For this example, we will use Spark to read in some Citibike data and write that data into an Oracle table.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1521658284132_-1572228355","id":"20170504-134249_302037388","dateCreated":"2018-03-21T18:51:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:547"},{"title":"Spark Scala to read CSV and register as Spark SQL temporary table","text":"%spark\n\n//a previous tutorial placed the csv file into /var/lib/zeppelin/citibike/201612-citibike-tripdata.csv\n\nval df = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").load(\"file:/var/lib/zeppelin/citibike/201612-citibike-tripdata.csv\")\n\n//cache the data frame for performance\ndf.cache()\n\n\nprintln(\"Here is the schema detected from the CSV\")\ndf.printSchema()\nprintln(\"..\")\n\nprintln(\"# of rows: %s\".format(\n  df.count() \n)) \nprintln(\"..\")\n\ndf.createOrReplaceTempView(\"bike_trips_csvtemp\")\nprintln(\"done\")","user":"anonymous","dateUpdated":"2018-03-23T14:07:33+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521658284132_-1572228355","id":"20170701-170923_1753081660","dateCreated":"2018-03-21T18:51:24+0000","dateStarted":"2018-03-23T14:07:33+0000","dateFinished":"2018-03-23T14:07:46+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:548","errorMessage":""},{"title":"Spark to write a DataFrame to an Oracle table","text":"%spark\n\n\n\n// To make sure we have Oracle friendly column names, lets select against our Spark SQL temp table and rename columns\nprintln(\"Renaming column names via bike_query...\")\nval bike_query = sqlContext.sql(\"\"\"select `Trip Duration` TRIPDURATION, \n`Start Time` STARTTIME, \n`Stop Time` STOPTIME, \n`Start Station ID` STARTSTATIONID, \n`Start Station Name` STARTSTATIONNAME, \n`Start Station Latitude` STARTSTATIONLATITUDE, \n`Start Station Longitude` STARTSTATIONLONGITUDE, \n`End Station ID` ENDSTATIONID, \n`End Station Name` ENDSTATIONNAME, \n`End Station Latitude` ENDSTATIONLATITUDE, \n`End Station Longitude` ENDSTATIONLONGITUDE, \n`Bike ID` BIKEID, \n`User Type` USERTYPE, \n`Birth Year` BIRTHYEAR, \n`Gender` GENDER \n from bike_trips_csvtemp\"\"\")\n \nbike_query.show()\nbike_query.printSchema()\n\n\nimport org.apache.spark.sql.SaveMode\n//possible SaveModes are SaveMode.Append, SaveMode.Overwrite, SaveMode.ErrorIfExists, SaveMode.Ignore\n\n\nprintln(\"Writing Spark DataFrame to Oracle Database.  This may take a few minutes.\")\nbike_query.write\n   .mode(SaveMode.Overwrite)\n   .jdbc(url,\"CITIBIKE_ORCL\",prop)\n\n\n//your Spark dataframe needs to use valid Oracle column names (i.e. no spaces, no reserved words, etc).  If you need to rename dataframe fields, you can do operations like this\n//val newdDF=oldDF.withColumnRenamed(\"Birth Year\",\"BirthYear\")\n\n\nprintln(\"done\")","user":"anonymous","dateUpdated":"2018-03-23T14:07:48+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521658284132_-1572228355","id":"20170813-182552_758023941","dateCreated":"2018-03-21T18:51:24+0000","dateStarted":"2018-03-23T14:07:48+0000","dateFinished":"2018-03-23T14:08:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:549","errorMessage":""},{"title":"JDBC to query Oracle and see the new Oracle table","text":"%jdbc(adwc-admin)\nselect * from dave.CITIBIKE_ORCL\nwhere rownum<20","user":"anonymous","dateUpdated":"2018-03-23T14:08:40+0000","config":{"editorSetting":{"editOnDblClick":false,"language":"text"},"colWidth":12,"editorMode":"ace/mode/text","editorHide":false,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521658284133_-1572613104","id":"20170504-145612_585260865","dateCreated":"2018-03-21T18:51:24+0000","dateStarted":"2018-03-23T14:08:40+0000","dateFinished":"2018-03-23T14:08:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:550","errorMessage":""},{"dateUpdated":"2018-03-21T18:51:24+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521658284133_-1572613104","id":"20170414-131833_1025546137","dateCreated":"2018-03-21T18:51:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:552"}],"name":"Extras/Working with ADWC","id":"2DC2HKTQE","angularObjects":{"2D9YK2E72:shared_process":[],"2DBD6T1A5:shared_process":[],"2DAX9FGJG:shared_process":[],"2D9DWMGPJ:shared_process":[],"2D9M4ANGW:shared_process":[],"2D93S9FJZ:shared_process":[],"2D8AY4QB2:shared_process":[],"2C4U48MY3_spark2:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}