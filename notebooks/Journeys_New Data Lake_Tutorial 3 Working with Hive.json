{"paragraphs":[{"text":"%md\n# Tutorial 3: Working with Hive\n\nThis tutorial was built for BDCS-CE version 17.4.1 as part of the New Data Lake User Journey: <a href=\"https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake\" target=\"_blank\">here</a>.  Questions and feedback about the tutorial: <david.bayard@oracle.com>\n\n    Be sure to run the previous Tutorial: \"Citi Bike New York Introduction and Setup\"\n\nThis tutorial will illustrate using the Shell interpreter to run a few hive command lines and using the JDBC interpreter to run a few hive queries.\n\n## Contents\n\n+ About Apache Hive\n+ Create a Hive Table with the shell interpreter \n+ Running simple queries (select, show databases, show tables)\n+ Query a simple Hive table and graph the results\n+ Next Steps\n\n\nAs a reminder, the documentation for BDCS-CE can be found  <a href=\"https://docs.oracle.com/cloud/latest/big-data-compute-cloud/index.html\" target=\"_new\">here</a>","dateUpdated":"2017-11-17T15:09:40+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Tutorial 3: Working with Hive</h1>\n<p>This tutorial was built for BDCS-CE version 17.4.1 as part of the New Data Lake User Journey: <a href=\"https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake\" target=\"_blank\">here</a>. Questions and feedback about the tutorial: <a href=\"mailto:&#100;&#97;&#118;&#x69;&#100;&#46;&#x62;&#97;&#x79;&#x61;&#x72;&#x64;&#64;&#x6f;&#114;&#97;c&#108;&#x65;&#x2e;c&#x6f;&#109;\">&#100;&#97;&#118;&#x69;&#100;&#46;&#x62;&#97;&#x79;&#x61;&#x72;&#x64;&#64;&#x6f;&#114;&#97;c&#108;&#x65;&#x2e;c&#x6f;&#109;</a></p>\n<pre><code>Be sure to run the previous Tutorial: &quot;Citi Bike New York Introduction and Setup&quot;\n</code></pre>\n<p>This tutorial will illustrate using the Shell interpreter to run a few hive command lines and using the JDBC interpreter to run a few hive queries.</p>\n<h2>Contents</h2>\n<ul>\n  <li>About Apache Hive</li>\n  <li>Create a Hive Table with the shell interpreter</li>\n  <li>Running simple queries (select, show databases, show tables)</li>\n  <li>Query a simple Hive table and graph the results</li>\n  <li>Next Steps</li>\n</ul>\n<p>As a reminder, the documentation for BDCS-CE can be found <a href=\"https://docs.oracle.com/cloud/latest/big-data-compute-cloud/index.html\" target=\"_new\">here</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1510931380629_1094133191","id":"20160524-153718_1397925708","dateCreated":"2017-11-17T15:09:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2775"},{"text":"%md\n# About Apache Hive\n\nApache Hive is a component that enables the use of SQL against a variety of data, including data stored locally, in the Hadoop Distributed File System (HDFS), and in the Object Store.\n\nIn this tutorial, we will use Hive to enable SQL queries against our Citi Bike .csv dataset.\n\nLearn more about Hive <a href=\"https://hive.apache.org/\" target=\"_blank\">here</a>","dateUpdated":"2017-11-17T15:09:40+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>About Apache Hive</h1>\n<p>Apache Hive is a component that enables the use of SQL against a variety of data, including data stored locally, in the Hadoop Distributed File System (HDFS), and in the Object Store.</p>\n<p>In this tutorial, we will use Hive to enable SQL queries against our Citi Bike .csv dataset.</p>\n<p>Learn more about Hive <a href=\"https://hive.apache.org/\" target=\"_blank\">here</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1510931380631_1094902688","id":"20170811-132159_567952665","dateCreated":"2017-11-17T15:09:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2776"},{"text":"%md\n# Create a Hive Table\n\nOur first step will be to define a Hive table on top of our CSV file.  We will show 2 variations.  The first example leverages a \"managed\" table where Hive manages the storage details (internally Hive will leverage HDFS storage).  The second example leverages an \"external\" table where Hive will read the data directly from the Object Store.\n\nWe will use the hive command line running in the shell interpreter to create our tables.","dateUpdated":"2017-11-17T15:09:40+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Create a Hive Table</h1>\n<p>Our first step will be to define a Hive table on top of our CSV file. We will show 2 variations. The first example leverages a &ldquo;managed&rdquo; table where Hive manages the storage details (internally Hive will leverage HDFS storage). The second example leverages an &ldquo;external&rdquo; table where Hive will read the data directly from the Object Store.</p>\n<p>We will use the hive command line running in the shell interpreter to create our tables.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1510931380631_1094902688","id":"20170811-132429_1558998735","dateCreated":"2017-11-17T15:09:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2777"},{"title":"Create a \"managed\" Bike Table from the CSV file (takes about 2 minutes)","text":"%sh \n\nDIRECTORY=citibike\nFILENAME=201612-citibike-tripdata\n\necho \"Data Set name (remove .zip or .csv)  :\" $FILENAME\necho \"-----------------------------------------------------------------\"\necho \"..\"\necho \"If this paragraph hangs on the SELECT count(*) FROM bike_trips, see the next paragraph for an explanation/resolution....\"\necho \"..\"\n\ncd $DIRECTORY\n\n# run hive\nhive <<EOF\nDROP TABLE IF EXISTS bike_trips;\n\nCREATE TABLE bike_trips ( \nTripDuration int,\nStartTime timestamp,\nStopTime timestamp,\nStartStationID string,\nStartStationName string,\nStartStationLatitude string,\nStartStationLongitude string,\nEndStationID string,\nEndStationName string,\nEndStationLatitude string,\nEndStationLongitude string,\nBikeID int,\nUserType string,\nBirthYear int, \nGender int\n) \nROW FORMAT delimited \nFIELDS TERMINATED BY ',' ;\n\nLOAD DATA LOCAL INPATH '$FILENAME.nh.csv' into table bike_trips;\n\ncreate table bike_trips_small as select * from bike_trips limit 50000;\n\nSELECT count(*) FROM bike_trips;\n\nexit;\n\nEOF\n","dateUpdated":"2017-11-17T15:09:40+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"false","language":"sh"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"results":{"0":{"graph":{"mode":"table","height":471.15,"optionOpen":false}}},"graph":{"mode":"table","height":431,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{"DataFile":"base_bike_nh_long.csv","localFS":"/tmp/hive-demo","data":"base_bike_nh.csv","prefix":"/tmp/hive-demo","CONTAINER":"citibike","hadoopFS":"/user/zeppelin/hive-demo","FILENAME":"201612-citibike-tripdata"},"forms":{}},"apps":[],"jobName":"paragraph_1510931380632_1092978944","id":"20170413-132012_1207762175","dateCreated":"2017-11-17T15:09:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2778"},{"text":"%md\n## If the above paragraph seems to hang...\n\nIf you created the smallest BDCS-CE cluster (using the OC2M shape), then the hadoop scheduling system (YARN) can sometimes run out of available slots (virtual cores) to run applications.  If the hive job takes more than 3 minutes, this is likely the cause.  The solution is this:\n\n+ Go to the Jobs tab\n+ Look for the a running/processing non-Hive job, such as the Thrift server or the SparkSession labelled \"Zeppelin\".  Using the pop-up menu to the right of the job, choose to Abort the job.\n+ Then navigate back to this notebook\n","dateUpdated":"2017-11-17T15:09:40+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>If the above paragraph seems to hang&hellip;</h2>\n<p>If you created the smallest BDCS-CE cluster (using the OC2M shape), then the hadoop scheduling system (YARN) can sometimes run out of available slots (virtual cores) to run applications. If the hive job takes more than 3 minutes, this is likely the cause. The solution is this:</p>\n<ul>\n  <li>Go to the Jobs tab</li>\n  <li>Look for the a running/processing non-Hive job, such as the Thrift server or the SparkSession labelled &ldquo;Zeppelin&rdquo;. Using the pop-up menu to the right of the job, choose to Abort the job.</li>\n  <li>Then navigate back to this notebook</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1510931380632_1092978944","id":"20170811-133701_1067316681","dateCreated":"2017-11-17T15:09:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2779"},{"title":"Create an \"external\" table against the Object Store (takes about 3 minutes)","text":"%sh\n\nCONTAINER=journeyC\nDIRECTORY=citibike\nFILENAME=201612-citibike-tripdata\n\necho \"Object Storage Container Name        :\" $CONTAINER\necho \"Data Set name (remove .zip or .csv)  :\" $FILENAME\necho \"-----------------------------------------------------------------\"\n\n\n# run hive\nhive  <<EOF\nDROP TABLE bike_trips_objectstore;\n\nCREATE external TABLE bike_trips_objectstore ( \nTripDuration int,\nStartTime timestamp,\nStopTime timestamp,\nStartStationID string,\nStartStationName string,\nStartStationLatitude string,\nStartStationLongitude string,\nEndStationID string,\nEndStationName string,\nEndStationLatitude string,\nEndStationLongitude string,\nBikeID int,\nUserType string,\nBirthYear int, \nGender int\n) \nROW FORMAT delimited \nFIELDS TERMINATED BY ',' \nlocation 'oci://test01bdc01@orasejapan/citibike/modified/';\n\n\nSELECT count(*) FROM bike_trips_objectstore;\n\nexit;\n\nEOF\n","dateUpdated":"2017-11-17T15:09:40+0000","config":{"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":"false"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"results":{"0":{"graph":{"mode":"table","height":547.15,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510931380633_1092594195","id":"20170811-133146_1280326847","dateCreated":"2017-11-17T15:09:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2780"},{"title":"Show the Hive tables defined","text":"%sh\n\nhive  <<EOF\nshow tables;\nEOF\n","dateUpdated":"2017-11-17T15:09:40+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"false","language":"sh"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":true,"keys":[{"name":"gender","index":0,"aggr":"sum"}],"values":[{"name":"a.trip_count","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"gender","index":0,"aggr":"sum"},"yAxis":{"name":"a.trip_count","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510931380633_1092594195","id":"20170511-135148_457890564","dateCreated":"2017-11-17T15:09:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2781"},{"title":"Sample query via Hive Command Line","text":"%sh\n\nhive  <<EOF\nselect \n case when a.gender=1 then 'Male' when a.gender=2 then 'Female' else 'unknown' end gender ,\n        a.trip_count \nfrom (select gender, count(*) trip_count from bike_trips\ngroup by gender) a;\nEOF","dateUpdated":"2017-11-17T15:09:40+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"false","language":"sh"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"results":{"0":{"graph":{"mode":"table","height":454,"optionOpen":false}}},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510931380634_1093748442","id":"20170425-120754_372055516","dateCreated":"2017-11-17T15:09:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2782"},{"text":"%md\n## Working with the JDBC(Hive) interpreter\n\nZeppelin includes a JDBC interpreter that allows you run a query as a paragraph and do some nice formating of the results.  In BDCS-CE, the JDBC interpreter has been pre-configured to connect to Hive.\n\n\nYou can work with the JDBC interpreter and connect to Hive like this:\n\n    %jdbc(hive)\n    select * from my_table;\n","dateUpdated":"2017-11-17T15:09:40+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Working with the JDBC(Hive) interpreter</h2>\n<p>Zeppelin includes a JDBC interpreter that allows you run a query as a paragraph and do some nice formating of the results. In BDCS-CE, the JDBC interpreter has been pre-configured to connect to Hive.</p>\n<p>You can work with the JDBC interpreter and connect to Hive like this:</p>\n<pre><code>%jdbc(hive)\nselect * from my_table;\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1510931380634_1093748442","id":"20170811-133837_19001353","dateCreated":"2017-11-17T15:09:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2783"},{"text":"%jdbc(hive)\nshow tables","dateUpdated":"2017-11-17T15:09:40+0000","config":{"colWidth":6,"editorMode":"ace/mode/text","results":{},"enabled":true,"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510931380635_1093363693","id":"20170811-134137_1638313959","dateCreated":"2017-11-17T15:09:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2784"},{"text":"%jdbc(hive)\ndescribe bike_trips","dateUpdated":"2017-11-17T15:09:40+0000","config":{"colWidth":6,"editorMode":"ace/mode/text","results":{},"enabled":true,"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510931380635_1093363693","id":"20170811-134205_551795930","dateCreated":"2017-11-17T15:09:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2785"},{"text":"%jdbc(hive)\nshow create table bike_trips","dateUpdated":"2017-11-17T15:09:40+0000","config":{"colWidth":6,"editorMode":"ace/mode/text","results":{"0":{"graph":{"mode":"table","height":747,"optionOpen":false}}},"enabled":true,"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510931380635_1093363693","id":"20170811-134220_795710812","dateCreated":"2017-11-17T15:09:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2786"},{"title":"Query to show Data as a Pie Chart (Riders by Gender)","text":"%jdbc(hive)\n\nselect \n case when a.gender=1 then 'Male' when a.gender=2 then 'Female' else 'Unknown' end gender,  a.trip_count \nfrom (select gender, count(*) trip_count from bike_trips\ngroup by gender) a","dateUpdated":"2017-11-17T15:09:40+0000","config":{"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","title":true,"results":{"0":{"graph":{"mode":"pieChart","height":300,"optionOpen":false},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510931380636_1091439948","id":"20170811-134230_836345512","dateCreated":"2017-11-17T15:09:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2787"},{"title":"Query to Show a Chart with Stacks or Groupings (Riders by Gender by Day)","text":"%jdbc(hive)\n\nselect gender, dayofweek, count(*)\nfrom (    select date_format(`StartTime`,\"E\") dayofweek,\n          case when gender=1 then 'Male' when gender=2 then 'Female' else 'Unknown' end gender \nfrom bike_trips) bike_times \ngroup by gender, dayofweek","dateUpdated":"2017-11-17T15:09:40+0000","config":{"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","title":true,"results":{"0":{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"setting":{"multiBarChart":{"stacked":true}},"commonSetting":{},"keys":[{"name":"dayofweek","index":1,"aggr":"sum"}],"groups":[{"name":"gender","index":0,"aggr":"sum"}],"values":[{"name":"_c2","index":2,"aggr":"sum"}]},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510931380636_1091439948","id":"20170811-134245_904971270","dateCreated":"2017-11-17T15:09:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2788"},{"title":"Hive Query to Show a Chart (Age Distribution)","text":"%jdbc(hive)\n\nselect (2016 - a.birthyear) age, count(*) number_trips from bike_trips as a\nwhere birthyear is not null\ngroup by birthyear\n","dateUpdated":"2017-11-17T15:09:40+0000","config":{"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","title":true,"results":{"0":{"graph":{"mode":"stackedAreaChart","height":300,"optionOpen":false,"setting":{"stackedAreaChart":{"style":"stack"}},"commonSetting":{},"keys":[{"name":"age","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"number_trips","index":1,"aggr":"sum"}]},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510931380637_1091055199","id":"20170811-134255_220668446","dateCreated":"2017-11-17T15:09:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2789"},{"text":"%md\n# Next Steps\n\nSo far, we\n\n1) have downloaded a Citi Bike data zip and created csv files. Then we stored the data into the Object Storage. \n2) set up a Hive table using the Hive command line\n3) queried Hive via Zepellin's JDBC(Hive) Interpreter\n\nIn the next tutorial we use Spark for processing and query. ","dateUpdated":"2017-11-17T15:09:40+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Next Steps</h1>\n<p>So far, we</p>\n<p>1) have downloaded a Citi Bike data zip and created csv files. Then we stored the data into the Object Storage.<br/>2) set up a Hive table using the Hive command line<br/>3) queried Hive via Zepellin&rsquo;s JDBC(Hive) Interpreter</p>\n<p>In the next tutorial we use Spark for processing and query.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1510931380637_1091055199","id":"20170512-090716_2012584955","dateCreated":"2017-11-17T15:09:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2790"},{"text":"%md\n### Tip: Running Hive and Zeppelin/Spark on a 2 OCPUs instance\n\nSome of you may be running this tutorial on the smallest configuration size (2 OCPUs).  If you are using this small shape, there can be scenarios where your Hive commands get \"Accepted\" but do not progress to \"Processing\".  You can see this behavior in the Jobs tab.  If this happens, then your Hive commands will hang indefinitely.  The reason this happens is that with 2 OCPUs, all of the available processing threads in the YARN resource manager can be occupied by other jobs.  There is a simple solution if you see this happen:  simply go to the Jobs tab, find the \"Zeppelin\" job or the \"Thrift server\" job, and abort it.  You will find that your Hive job will then move from \"Accepted\" to \"Processing\" and then finish.  The \"Zeppelin\" job will relaunch itself automatically the next time you run something with the Spark Interpreter inside Zeppelin.\n\nAn alternate solution is to add another node to your BDCS-CE cluster.  BDCS-CE is scalable, so you can add a node which will provide additional processing threads for YARN to work with.  You can also drop the node later if you want.","dateUpdated":"2017-11-17T15:09:40+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Tip: Running Hive and Zeppelin/Spark on a 2 OCPUs instance</h3>\n<p>Some of you may be running this tutorial on the smallest configuration size (2 OCPUs). If you are using this small shape, there can be scenarios where your Hive commands get &ldquo;Accepted&rdquo; but do not progress to &ldquo;Processing&rdquo;. You can see this behavior in the Jobs tab. If this happens, then your Hive commands will hang indefinitely. The reason this happens is that with 2 OCPUs, all of the available processing threads in the YARN resource manager can be occupied by other jobs. There is a simple solution if you see this happen: simply go to the Jobs tab, find the &ldquo;Zeppelin&rdquo; job or the &ldquo;Thrift server&rdquo; job, and abort it. You will find that your Hive job will then move from &ldquo;Accepted&rdquo; to &ldquo;Processing&rdquo; and then finish. The &ldquo;Zeppelin&rdquo; job will relaunch itself automatically the next time you run something with the Spark Interpreter inside Zeppelin.</p>\n<p>An alternate solution is to add another node to your BDCS-CE cluster. BDCS-CE is scalable, so you can add a node which will provide additional processing threads for YARN to work with. You can also drop the node later if you want.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1510931380638_1092209446","id":"20170512-135634_1459754506","dateCreated":"2017-11-17T15:09:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2791"},{"text":"%md\n### Change Log\nSeptember 7, 2017 - Confirmed it works on 17.3.5-20.  Tweaked the hive managed table script\nAugust 23, 2017 - A few minor tweaks\nAugust 13, 2017 - Confirmed it works on 17.3.3-20\nAugust 11, 2017 - Journey v2\nJuly 28, 2017 - Confirmed that it works on 17.3.1-20","dateUpdated":"2017-11-17T15:09:40+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Change Log</h3>\n<p>September 7, 2017 - Confirmed it works on 17.3.5-20. Tweaked the hive managed table script<br/>August 23, 2017 - A few minor tweaks<br/>August 13, 2017 - Confirmed it works on 17.3.3-20<br/>August 11, 2017 - Journey v2<br/>July 28, 2017 - Confirmed that it works on 17.3.1-20</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1510931380638_1092209446","id":"20170616-134351_1883942168","dateCreated":"2017-11-17T15:09:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2792"},{"text":"%md\n","dateUpdated":"2017-11-17T15:09:40+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":"true"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510931380639_1091824697","id":"20170728-130618_1284552180","dateCreated":"2017-11-17T15:09:40+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2793"}],"name":"Journeys/New Data Lake/Tutorial 3 Working with Hive","id":"2CYH54JY9","angularObjects":{"2CXJR3AHX:shared_process":[],"2CXKU133T:shared_process":[],"2D19V2229:shared_process":[],"2CZAK1K32:shared_process":[],"2D163KRC5:shared_process":[],"2CZVT6YPM:shared_process":[],"2CY7WZWMD:shared_process":[],"2C4U48MY3_spark2:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}