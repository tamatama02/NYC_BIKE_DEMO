{"paragraphs":[{"text":"%md\n# DataSci Tutorial 2: Using R with Zeppelin\n\nThis tutorial was built for BDCS-CE version 17.4.1 as part of the Data Science Acceleration User Journey: <a href=\"https://oracle.github.io/learning-library/workshops/journey3-data-science/\" target=\"_blank\">here</a>.  Questions and feedback about the tutorial: <david.bayard@oracle.com>\n\n    Be sure you previously ran the Tutorial \"Setup R, SparkR, RStudio Server\".\n\nThis tutorial provides some examples of using R and SparkR in Zeppelin notebooks.  It will show:\n\n+ How to query a hive table from R\n+ How to read data directly from the Object Store\n+ How to convert a R data.frame into a Spark Temporary Table and query it with SparkSQL\n+ Machine Learning with R and Spark\n+ Save results back to the Object Store\n\n","dateUpdated":"2017-11-17T15:09:38+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>DataSci Tutorial 2: Using R with Zeppelin</h1>\n<p>This tutorial was built for BDCS-CE version 17.4.1 as part of the Data Science Acceleration User Journey: <a href=\"https://oracle.github.io/learning-library/workshops/journey3-data-science/\" target=\"_blank\">here</a>. Questions and feedback about the tutorial: <a href=\"mailto:&#100;a&#118;&#x69;&#x64;&#x2e;&#x62;&#97;&#x79;&#x61;r&#100;&#x40;&#111;r&#97;&#99;&#108;e&#46;c&#x6f;&#109;\">&#100;a&#118;&#x69;&#x64;&#x2e;&#x62;&#97;&#x79;&#x61;r&#100;&#x40;&#111;r&#97;&#99;&#108;e&#46;c&#x6f;&#109;</a></p>\n<pre><code>Be sure you previously ran the Tutorial &quot;Setup R, SparkR, RStudio Server&quot;.\n</code></pre>\n<p>This tutorial provides some examples of using R and SparkR in Zeppelin notebooks. It will show:</p>\n<ul>\n  <li>How to query a hive table from R</li>\n  <li>How to read data directly from the Object Store</li>\n  <li>How to convert a R data.frame into a Spark Temporary Table and query it with SparkSQL</li>\n  <li>Machine Learning with R and Spark</li>\n  <li>Save results back to the Object Store</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1510931378996_48000931","id":"20170811-183847_958683840","dateCreated":"2017-11-17T15:09:38+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2929"},{"text":"%md\n# About R and Zeppelin\n\nZeppelin includes an interpreter that is integrated with R and SparkR.  You can find some details about Zeppelin and R <a href=\"https://zeppelin.apache.org/docs/0.7.0/interpreter/r.html\" target=\"_blank\">here</a>.  You can find some details about SparkR <a href=\"https://spark.apache.org/docs/2.1.0/sparkr.html\" target=\"_blank\">here</a>.\n\nThese examples focuses on using R to work with Spark features like DataFrames.  As the SparkR documentation writes, \"A SparkDataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R, but with richer optimizations under the hood. SparkDataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing local R data frames\".  This tutorial will demonstrate some of these capabilities.\n\n\n\n","dateUpdated":"2017-11-17T15:09:38+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>About R and Zeppelin</h1>\n<p>Zeppelin includes an interpreter that is integrated with R and SparkR. You can find some details about Zeppelin and R <a href=\"https://zeppelin.apache.org/docs/0.7.0/interpreter/r.html\" target=\"_blank\">here</a>. You can find some details about SparkR <a href=\"https://spark.apache.org/docs/2.1.0/sparkr.html\" target=\"_blank\">here</a>.</p>\n<p>These examples focuses on using R to work with Spark features like DataFrames. As the SparkR documentation writes, &ldquo;A SparkDataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R, but with richer optimizations under the hood. SparkDataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing local R data frames&rdquo;. This tutorial will demonstrate some of these capabilities.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1510931378997_47616182","id":"20170807-213713_1708624789","dateCreated":"2017-11-17T15:09:38+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2930"},{"text":"%md\n# Example of R querying Hive\n\nThis example shows how to use R to query the bike_trips hive table via SparkR features.\n","dateUpdated":"2017-11-17T15:09:38+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Example of R querying Hive</h1>\n<p>This example shows how to use R to query the bike_trips hive table via SparkR features.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1510931378998_48770429","id":"20170810-001234_1882189757","dateCreated":"2017-11-17T15:09:38+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2931"},{"title":"SparkR to query a Hive table","text":"%r\nresults <- sql(\"SELECT * from bike_trips\")\nhead(results)\n","dateUpdated":"2017-11-17T15:09:38+0000","config":{"tableHide":false,"editorSetting":{"language":"r","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/r","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510931378999_48385680","id":"20170810-001354_1251372462","dateCreated":"2017-11-17T15:09:38+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2932"},{"text":"%r\n# let's see what kind of class our results are...\nresults\n# It is a SparkDataFrame","dateUpdated":"2017-11-17T15:09:39+0000","config":{"colWidth":12,"editorMode":"ace/mode/r","results":{},"enabled":true,"editorSetting":{"language":"r"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510931378999_48385680","id":"20170811-185109_995138400","dateCreated":"2017-11-17T15:09:38+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2933"},{"text":"%md\n# Example of reading a CSV from Object Store\n\nThis example shows SparkR features to read a CSV from Object Store via Spark's DataSources mechanisms\n\n\n","dateUpdated":"2017-11-17T15:09:39+0000","config":{"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Example of reading a CSV from Object Store</h1>\n<p>This example shows SparkR features to read a CSV from Object Store via Spark&rsquo;s DataSources mechanisms</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1510931379000_46461936","id":"20170810-001731_1366310690","dateCreated":"2017-11-17T15:09:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2934"},{"text":"%r\nbiketrips <- read.df(\"oci://test01bdc01@orasejapan/citibike/raw/201612-citibike-tripdata.csv\", \"csv\", header = \"true\", inferSchema = \"true\", na.strings = \"NA\")\nhead(biketrips)\n","dateUpdated":"2017-11-17T15:09:39+0000","config":{"colWidth":12,"editorMode":"ace/mode/r","results":{},"enabled":true,"editorSetting":{"language":"r","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510931379001_46077187","id":"20170810-001850_158683843","dateCreated":"2017-11-17T15:09:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2935"},{"text":"%md\n# Example of making a R dataframe into a SparkSQL table\n\nHere is an example of converting a R data.frame into a Spark DataFrame and registering it as a Spark SQL table.  You can more examples like this <a href=\"https://rpubs.com/wendyu/sparkr\" target=\"_blank\">here</a>.\n","dateUpdated":"2017-11-17T15:09:39+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Example of making a R dataframe into a SparkSQL table</h1>\n<p>Here is an example of converting a R data.frame into a Spark DataFrame and registering it as a Spark SQL table. You can more examples like this <a href=\"https://rpubs.com/wendyu/sparkr\" target=\"_blank\">here</a>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1510931379001_46077187","id":"20170810-002107_611682017","dateCreated":"2017-11-17T15:09:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2936"},{"title":"Load an R data frame","text":"%r\ndata(iris)\niris","dateUpdated":"2017-11-17T15:09:39+0000","config":{"editorSetting":{"language":"r"},"colWidth":12,"editorMode":"ace/mode/r","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510931379002_47231434","id":"20170808-174600_955896680","dateCreated":"2017-11-17T15:09:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2937"},{"title":"SparkR code to register an R dataframe as a SparkSQL table","text":"%r\nirisDF <- as.DataFrame(iris)\nregisterTempTable(irisDF,\"iris\")\nirisDF\n","dateUpdated":"2017-11-17T15:09:39+0000","config":{"editorSetting":{"language":"r"},"colWidth":12,"editorMode":"ace/mode/r","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510931379002_47231434","id":"20170808-185507_1332635419","dateCreated":"2017-11-17T15:09:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2938"},{"title":"SparkSQL querying R data","text":"%sql\nselect * from iris\n\n","dateUpdated":"2017-11-17T15:09:39+0000","config":{"editorSetting":{"language":"sql"},"colWidth":12,"editorMode":"ace/mode/sql","title":true,"results":{"0":{"graph":{"mode":"scatterChart","height":300,"optionOpen":false,"setting":{"scatterChart":{"xAxis":{"name":"Sepal_Length","index":0,"aggr":"sum"},"yAxis":{"name":"Sepal_Width","index":1,"aggr":"sum"},"group":{"name":"Species","index":4,"aggr":"sum"}}}},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510931379003_46846685","id":"20170810-002439_1141063356","dateCreated":"2017-11-17T15:09:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2939"},{"text":"%md\n# Machine Learning with R and Spark\n\nThis example shows running a Spark machine learning algorithm - Generalized Linear Model (glm).\n\nWe will use our citibike data and model tripduration based on age and gender.\n","dateUpdated":"2017-11-17T15:09:39+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Machine Learning with R and Spark</h1>\n<p>This example shows running a Spark machine learning algorithm - Generalized Linear Model (glm).</p>\n<p>We will use our citibike data and model tripduration based on age and gender.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1510931379004_44922940","id":"20170810-004712_451598661","dateCreated":"2017-11-17T15:09:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2940"},{"title":"SparkR code to build generalized linear model (GLM) of tripduration based on gender and age","text":"%r\nageGender  <- sql(\"SELECT tripduration, (2016-birthyear) age, gender from bike_trips\")\ntraining <- dropna(ageGender)\n\nmodel <- glm(tripduration ~ age + gender,\n    family = \"gaussian\", data = training)\nsummary(model)","dateUpdated":"2017-11-17T15:09:39+0000","config":{"editorSetting":{"language":"r","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/r","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510931379004_44922940","id":"20170810-004706_1398119785","dateCreated":"2017-11-17T15:09:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2941"},{"title":"Check our predictions (not so good)","text":"%r\nfitted <- predict(model, training)\nregisterTempTable(fitted,\"fitted\")\ncompare <- sql(\"select prediction, tripduration, age, gender from fitted\")\nhead(compare)\n","dateUpdated":"2017-11-17T15:09:39+0000","config":{"editorSetting":{"language":"r"},"colWidth":12,"editorMode":"ace/mode/r","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510931379005_44538191","id":"20170810-010230_308509598","dateCreated":"2017-11-17T15:09:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2942"},{"title":"SparkSQL to view predictions","text":"%sql\nselect prediction, tripduration, gender, age from fitted\nlimit 100\n","dateUpdated":"2017-11-17T15:09:39+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/sql","title":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":true,"setting":{"lineChart":{},"scatterChart":{"xAxis":{"name":"prediction","index":0,"aggr":"sum"},"yAxis":{"name":"tripduration","index":1,"aggr":"sum"},"group":{"name":"gender","index":2,"aggr":"sum"}},"stackedAreaChart":{}},"keys":[],"groups":[],"values":[{"name":"tripduration","index":1,"aggr":"sum"},{"name":"prediction","index":0,"aggr":"sum"}],"commonSetting":{}},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510931379005_44538191","id":"20170808-202112_1798282605","dateCreated":"2017-11-17T15:09:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2943"},{"text":"%md\n# Example of writing a DataFrame back to Object Store\n\nThe follow shows an example of writing a DataFrame back to the Object Store.  We use the write.df method from SparkR.  It supports multiple source types (csv, json, parquet, etc).","dateUpdated":"2017-11-17T15:09:39+0000","config":{"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Example of writing a DataFrame back to Object Store</h1>\n<p>The follow shows an example of writing a DataFrame back to the Object Store. We use the write.df method from SparkR. It supports multiple source types (csv, json, parquet, etc).</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1510931379006_45692438","id":"20170811-191501_1517148443","dateCreated":"2017-11-17T15:09:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2944"},{"title":"R code to write our predictions back to the Object Store","text":"%r\n# Since we know the resulting file is small, we will do a repartition command to force Spark to write the output as a single file.  This is an optional step.\nfitted_singlepartition <- repartition(fitted,1)\nwrite.df(fitted_singlepartition, \"oci://test01bdc01@orasejapan/citibike/results/201612-fitted-projections\", source=\"csv\", mode=\"overwrite\")\n","dateUpdated":"2017-11-17T15:09:39+0000","config":{"editorSetting":{"language":"r","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/r","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510931379007_45307689","id":"20170811-190712_164268436","dateCreated":"2017-11-17T15:09:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2945"},{"title":"Explore the contents of the Object Store","text":"%sh\n# this command will show you the contents of the Object Store that were just written\nhadoop fs -ls oci://test01bdc01@orasejapan/citibike/results/201612-fitted-projections\n","dateUpdated":"2017-11-17T15:09:39+0000","config":{"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":"false"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510931379008_31071980","id":"20170807-214237_845297194","dateCreated":"2017-11-17T15:09:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2946"},{"text":"%md\n### Change Log\nNovember 16, 2017 - Confirmed it works with 17.4.1\nSeptember 12, 2017 - Confirmed it works with 17.3.5\nAugust 13, 2017 - Confirmed it works with BDCSCE 17.3.3-20\nAugust 11, 2017 - First version\n","dateUpdated":"2017-11-17T15:09:39+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Change Log</h3>\n<p>November 16, 2017 - Confirmed it works with 17.4.1<br/>September 12, 2017 - Confirmed it works with 17.3.5<br/>August 13, 2017 - Confirmed it works with BDCSCE 17.3.3-20<br/>August 11, 2017 - First version</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1510931379008_31071980","id":"20170811-191623_826620851","dateCreated":"2017-11-17T15:09:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2947"},{"text":"%md\n","dateUpdated":"2017-11-17T15:09:39+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":"true"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510931379008_31071980","id":"20170811-191958_1291497317","dateCreated":"2017-11-17T15:09:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2948"}],"name":"Journeys/DataSci/Tutorial 2 Using R with Zeppelin","id":"2CZJQSGM3","angularObjects":{"2CXJR3AHX:shared_process":[],"2CXKU133T:shared_process":[],"2D19V2229:shared_process":[],"2CZAK1K32:shared_process":[],"2D163KRC5:shared_process":[],"2CZVT6YPM:shared_process":[],"2CY7WZWMD:shared_process":[],"2C4U48MY3_spark2:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}